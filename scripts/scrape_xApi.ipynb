{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T07:17:01.630865Z",
     "start_time": "2025-05-07T07:17:00.492712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Requirements\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import yaml\n",
    "from utils.checkTweetsForDuplicates import DuplicateFinder\n",
    "from pprint import pprint\n",
    "from configparser import ConfigParser\n",
    "from random import randint"
   ],
   "id": "fffbd479a67bf527",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T07:17:01.666761Z",
     "start_time": "2025-05-07T07:17:01.657962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Debug mode for testing\n",
    "DEBUG_MODE = True"
   ],
   "id": "f6098b2a2f22e406",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T07:17:03.999619Z",
     "start_time": "2025-05-07T07:17:03.989983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reading api_key from config.ini\n",
    "api_key: str | None = None\n",
    "try:\n",
    "    print(\"Reading api-key from config.ini ...\")\n",
    "    config = ConfigParser()\n",
    "    config.read('../conf/config.ini') # adjust the path to config.ini\n",
    "    api_key = config['TWITTERAPI']['API_KEY']\n",
    "    print(f\"API Key gefunden: {api_key[:5]} ...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to read credentials. Exit with error:\")\n",
    "    print(e)"
   ],
   "id": "45db1b57136b4e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading api-key from config.ini ...\n",
      "API Key gefunden: 1228a ...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:30:32.168199Z",
     "start_time": "2025-05-07T08:30:32.131462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Declare vars\n",
    "if api_key is None:\n",
    "    raise ValueError(\"Could not read api key.\")\n",
    "\n",
    "# Array to store tweets\n",
    "all_tweets = []\n",
    "\n",
    "# Parameters for api request\n",
    "headers = {\"X-API-Key\": api_key}\n",
    "url = \"https://api.twitterapi.io/twitter/tweet/advanced_search\"\n",
    "cursor = \"\"\n",
    "\n",
    "# counter\n",
    "i = 1"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T07:17:31.962600Z",
     "start_time": "2025-05-07T07:17:31.950197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating cursor file\n",
    "\n",
    "# Storing latest cursor position\n",
    "actual_cursor = \"DAADDAABCgABGp0h4-_aYKIKAAIaN7kUXtZB8QAIAAIAAAACCAADAAAAAAgABAAAARsKAAUaoHI5EIAnEAoABhqgcjkQVKpAAAA\"\n",
    "cursor_location = \"../utils/cursor.yml\"\n",
    "\n",
    "if not os.path.exists(\"../utils\"):\n",
    "    print(\"No previous cursor stored. Making file and storing last location ...\")\n",
    "    os.mkdir(\"../utils\")\n",
    "\n",
    "# If existing, read file content\n",
    "try:\n",
    "    with open(cursor_location, 'r') as file:\n",
    "        cursor_data = yaml.safe_load(file) or {}\n",
    "except FileNotFoundError:\n",
    "    cursor_data = {}\n",
    "\n",
    "# Construct new cursor data\n",
    "page_key = \"page_284\"\n",
    "page_data = {\n",
    "    \"tweets_per_page\": 20,\n",
    "    \"total_tweet_count\": 5668,\n",
    "    \"next_page\": actual_cursor\n",
    "}\n",
    "\n",
    "if page_key in cursor_data:\n",
    "    print(f\"Page '{page_key}' already in file. Next page @ ...{actual_cursor[-10:]}\")\n",
    "else:\n",
    "    # Add entry to (existing) data\n",
    "    cursor_data[page_key] = page_data\n",
    "    print(\"Adding following cursor data to file:\")\n",
    "    pprint(cursor_data)\n",
    "\n",
    "    with open(cursor_location, \"w\", encoding='utf-8') as f:\n",
    "        yaml.dump(cursor_data, f, allow_unicode=True, sort_keys=True)\n",
    "        print(\"Adding data successful. Continuing ...\")"
   ],
   "id": "d827e307cecc899b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 'page_284' already in file. Next page @ ...jkQVKpAAAA\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:29:59.612474Z",
     "start_time": "2025-05-07T08:29:58.179701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# https request\n",
    "# Endless loop based on has_next_page - break when false\n",
    "while True:\n",
    "    try:\n",
    "        print(f\"[Start] Starting new scraping interation.\")\n",
    "        print(\"Reading cursor.yml to set new cursor ...\")\n",
    "        try:\n",
    "            with open(cursor_location, 'r') as file:\n",
    "                cursor_data = yaml.safe_load(file) or {}\n",
    "                last_page = max((page for page in cursor_data.keys() if page.startswith(\"page_\")), default=None)\n",
    "                cursor = cursor_data[last_page].get('next_page', '')\n",
    "                print(f\"[Success] Starting new request with cursor: ...{cursor[-10:]}\")\n",
    "        except Exception as e:\n",
    "            print(\"[Error] Failed to read cursor data. Exiting ...\")\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "        # Make https request\n",
    "        print(\"Making request ...\")\n",
    "        response = requests.request(\n",
    "            \"GET\",\n",
    "            url,\n",
    "            headers=headers,\n",
    "            params={\n",
    "                \"query\": \"from:elonmusk since:2023-05-01 until:2025-05-01 -is:retweet\",\n",
    "                \"cursor\": cursor\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Debugging infos\n",
    "            print(f\"[Success] Status Code: {response.status_code}\")\n",
    "            print(\"Processing response ...\")\n",
    "\n",
    "            # throw exception for http error\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Store response\n",
    "            json_data = response.json()\n",
    "\n",
    "            # Extract necessary vars\n",
    "            tweets = json_data.get(\"tweets\", [])\n",
    "            tweet_ids = [tweet[\"id\"] for tweet in tweets]\n",
    "            has_next_page = json_data.get(\"has_next_page\")\n",
    "            next_cursor = json_data.get(\"next_cursor\")\n",
    "\n",
    "            # Store tweets and get tweet count\n",
    "            prev_tweet_count = len(all_tweets)\n",
    "            all_tweets.extend(tweets)\n",
    "            new_tweet_count = len(all_tweets) - prev_tweet_count\n",
    "            print(f\"- New Tweets in iteration {i}: {new_tweet_count}\")\n",
    "            print(f\"- Total Tweet count: {len(all_tweets)}\")\n",
    "\n",
    "            # Construct new cursor data\n",
    "            print(\"Appending metadata to cursor.yml ...\")\n",
    "            page_key = f\"page_{i + 284}\"\n",
    "            page_data = {\n",
    "                \"has_next_page\": has_next_page,\n",
    "                \"next_page\": next_cursor,\n",
    "                \"tweets_per_page\": new_tweet_count,\n",
    "                \"ids\": f'[{\", \".join(str(id) for id in tweet_ids)}]'\n",
    "            }\n",
    "\n",
    "            # Add page_data to cursor.yml\n",
    "            if page_key in cursor_data:\n",
    "                print(f\"[Error] Page '{page_key}' already in file. Next page @ ...{actual_cursor[-10:]}\")\n",
    "            else:\n",
    "                # Add entry to (existing) data\n",
    "                cursor_data[page_key] = page_data\n",
    "                print(\"[Success] Adding following cursor data to file:\")\n",
    "                pprint(cursor_data)\n",
    "\n",
    "                with open(cursor_location, \"w\", encoding='utf-8') as f:\n",
    "                    yaml.dump(cursor_data, f, allow_unicode=True, sort_keys=True)\n",
    "                    print(\"[Success] Updated cursor.yml file. Continuing ...\")\n",
    "\n",
    "            # Terminating loop if has_next_page is false or DEBUG_MODE is active\n",
    "            if not has_next_page:\n",
    "                print(f\"Got {new_tweet_count} tweets. Finished.\")\n",
    "                break\n",
    "\n",
    "            if DEBUG_MODE:\n",
    "                debug_mode_tweets = json_data[\"tweets\"][new_tweet_count - 1]\n",
    "                print(f'DEBUG_MODE active: stopping after first page. Got {new_tweet_count} tweets:')\n",
    "                #pprint(debug_mode_tweets)\n",
    "                i += 1\n",
    "                break\n",
    "\n",
    "            # Prepare the next iteration\n",
    "            print(f\"Next page detected @ '{next_cursor[:10]}'. Updating cursor ...\")\n",
    "            cursor = next_cursor\n",
    "            i += 1\n",
    "\n",
    "            # Wait random time to avoid rate limits\n",
    "            delay = randint(1, 5)\n",
    "            print(f\"Starting new iteration in {delay} seconds ...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error when requesting and processing tweets:\")\n",
    "        print(e)\n",
    "        break\n",
    "\n"
   ],
   "id": "c7eb9bb6aafe3651",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Start] Starting new scraping interation.\n",
      "Reading cursor.yml to set new cursor ...\n",
      "[Success] Starting new request with cursor: ...jkQVKpAAAA\n",
      "Making request ...\n",
      "[Success] Status Code: 200\n",
      "Processing response ...\n",
      "- New Tweets in iteration 4: 20\n",
      "- Total Tweet count: 80\n",
      "Appending metadata to cursor.yml ...\n",
      "[Success] Adding following cursor data to file:\n",
      "{'page_284': {'next_page': 'DAADDAABCgABGp0h4-_aYKIKAAIaN7kUXtZB8QAIAAIAAAACCAADAAAAAAgABAAAARsKAAUaoHI5EIAnEAoABhqgcjkQVKpAAAA',\n",
      "              'total_tweet_count': 5668,\n",
      "              'tweets_per_page': 20},\n",
      " 'page_288': {'has_next_page': True,\n",
      "              'ids': '[1889181769359937618, 1889181253137567767, '\n",
      "                     '1889180403623219298, 1889179957110120704, '\n",
      "                     '1889178822509027328, 1889177218246406326, '\n",
      "                     '1889176999123362255, 1889176750497595586, '\n",
      "                     '1889176307642020353, 1889175889914564627, '\n",
      "                     '1889175583810114045, 1889174678536626402, '\n",
      "                     '1889172689073357278, 1889170672699478047, '\n",
      "                     '1889170135199392208, 1889113707377996067, '\n",
      "                     '1889113063212687566, 1889112769581982159, '\n",
      "                     '1889112657149440387, 1889103742340882647]',\n",
      "              'next_page': 'DAADDAABCgABGp0h4-_aYKIKAAIaN3HYQhbg1wAIAAIAAAACCAADAAAAAAgABAAAARwKAAUaoHI5EIAnEAoABhqgcjkQVIMwAAA',\n",
      "              'tweets_per_page': 20}}\n",
      "[Success] Updated cursor.yml file. Continuing ...\n",
      "DEBUG_MODE active: stopping after first page. Got 20 tweets:\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T07:44:11.781001Z",
     "start_time": "2025-05-07T07:44:10.959375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for duplicates in tweets\n",
    "data_location = \"../data/twitter/tweets.csv\"\n",
    "\n",
    "if not os.path.exists(data_location):\n",
    "    print(\"File does not exist. Exiting ...\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    has_duplicates = DuplicateFinder(data_location)\n",
    "    print(f\"Has duplicates: {has_duplicates}\")\n",
    "except Exception as e:\n",
    "    print(\"Could not call function. Exit with error:\")\n",
    "    print(e)\n",
    "\n",
    "print(\"\\nNext cursor after manually stopping scraping:\")\n",
    "print(cursor)"
   ],
   "id": "7afb0ce3b3bd13a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n",
      "Has duplicates: False\n",
      "\n",
      "Next cursor after manually stopping scraping:\n",
      "DAADDAABCgABGp0h4-_aYKIKAAIaN3HYQhbg1wAIAAIAAAACCAADAAAAAAgABAAAARwKAAUaoHI5EIAnEAoABhqgcjkQVIMwAAA\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:22:01.034042Z",
     "start_time": "2025-05-07T08:22:00.863248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Writing tweets to file\n",
    "def add_tweets_to_csv(tweets):\n",
    "    try:\n",
    "        print('Finished request. Converting results to file ...')\n",
    "\n",
    "        # Create the dir if it does not exist\n",
    "        if not os.path.exists(\"../data/twitter\"):\n",
    "            os.mkdir(\"../data/twitter\")\n",
    "\n",
    "        # Target location\n",
    "        filepath = \"../data/twitter/tweets.csv\"\n",
    "\n",
    "        # Store existing tweet ids as set and write all existing ones\n",
    "        existing_ids = set()\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\", newline='') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for row in reader:\n",
    "                    existing_ids.add(row[\"id\"])\n",
    "\n",
    "        # Open the file in append mode\n",
    "        with open(filepath, \"a\", encoding=\"utf-8\", newline='') as f:\n",
    "            if not all_tweets:\n",
    "                print(\"No tweets to write. Exiting ...\")\n",
    "                exit()\n",
    "\n",
    "            # Flatten tweet structure\n",
    "            def flatten_tweet(tweet):\n",
    "                if not tweet:\n",
    "                    return {}\n",
    "\n",
    "                flat = {}\n",
    "                for k, v in tweet.items():\n",
    "                    try:\n",
    "                        if isinstance(v, (dict, list)):\n",
    "                            flat[k] = json.dumps(v, ensure_ascii=False)\n",
    "                        else:\n",
    "                            flat[k] = v\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error when processing field {k}: {e}\")\n",
    "                        flat[k] = None\n",
    "                return flat\n",
    "\n",
    "            flattened_tweets = [flatten_tweet(tw) for tw in all_tweets]\n",
    "            if not flattened_tweets:\n",
    "                print(\"Did not find tweets to write. Exiting ...\")\n",
    "                exit()\n",
    "\n",
    "            # Derive col names from the first tweet\n",
    "            fieldnames = list(flattened_tweets[0].keys())\n",
    "\n",
    "            # Create a csv writer with fieldnames\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            if os.stat(filepath).st_size == 0:\n",
    "                writer.writeheader()\n",
    "\n",
    "            # Write new tweets to a file\n",
    "            new_count = 0\n",
    "            for tweet in flattened_tweets:\n",
    "                if tweet[\"id\"] not in existing_ids:\n",
    "                    writer.writerow(tweet)\n",
    "                    new_count += 1\n",
    "\n",
    "            print(f\"Wrote {new_count} new tweets to CSV.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Failed to convert tweets to file. Exit with error:')\n",
    "        print(e)"
   ],
   "id": "88109f6b7632dd53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished request. Converting results to file ...\n",
      "Failed to convert tweets to file. Exit with error:\n",
      "field larger than field limit (131072)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T22:06:16.793842Z",
     "start_time": "2025-05-01T22:06:16.789846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Working directory\n",
    "# If the previous cell says it cannot find the location, run this first\n",
    "# os.chdir(os.path.expanduser(\"~/PycharmProjects/COIN_Repo\")) # adjust path to root dir of project\n",
    "# os.getcwd() # Should be root dir now"
   ],
   "id": "9b677c4f219790a9",
   "outputs": [],
   "execution_count": 150
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
