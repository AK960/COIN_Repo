{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be002d53",
   "metadata": {},
   "source": [
    "<h1>TikTok Analysis - Part 1</h1>\n",
    "<h2><i>Data Preparation</i></h2>"
   ]
  },
  {
   "cell_type": "code",
   "id": "7cbe182e",
   "metadata": {},
   "source": [
    "### Imports ###\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "### Set necessary workspace variables ###\n",
    "\n",
    "# Set execution type (to avoid repeating resource intensive operations)\n",
    "#RUN_TYPE = 0 # set to 0 to avoid file creation process\n",
    "RUN_TYPE = 1 # set to 1 to perform emotion analysis file creation process\n",
    "#RUN_TYPE = 2 # set to 2 to perform topic analysis file creation process\n",
    "\n",
    "# Define Ekman's emotions \n",
    "ekman_emotions = ['anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "\n",
    "# Timezones\n",
    "eastern = pytz.timezone(\"US/Eastern\")\n",
    "european = pytz.timezone(\"Europe/Berlin\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec062195",
   "metadata": {},
   "source": [
    "### Read and transform data (total_engagement, combined_text, set_timezone) ###\n",
    "# Read dataframe\n",
    "if RUN_TYPE == 1:\n",
    "    df_tiktok = pd.read_excel('../data/tiktok/tiktok_transcript.xlsx').drop(columns=['Hashtag', 'URL', 'Author']).dropna()\n",
    "\n",
    "    # Extract total_engagement and combine text columns\n",
    "    df_tiktok['total_engagement'] = df_tiktok['Likes'] + df_tiktok['Comments'] + df_tiktok['Shares']\n",
    "    df_tiktok['combined_text'] = df_tiktok['CaptionCleaned'].fillna('') + ' ' + df_tiktok['Transcript'].fillna('')\n",
    "    df_tiktok.drop(columns=['Likes', 'Comments', 'Shares', 'Caption', 'CaptionCleaned', 'Transcript'], inplace=True)\n",
    "\n",
    "    # Convert 'Date' column to datetime format and set timezone\n",
    "    df_tiktok = df_tiktok.rename(columns={'Created': 'timestamp'})\n",
    "    df_tiktok['timestamp'] = pd.to_datetime(\n",
    "        df_tiktok['timestamp'],\n",
    "        format=\"%a %b %d %H:%M:%S %z %Y\",\n",
    "        errors='coerce'\n",
    "    )\n",
    "    df_tiktok['timestamp'] = df_tiktok['timestamp'].dt.tz_localize(eastern)\n",
    "\n",
    "    print(f\"Länge df: {len(df_tiktok)}\")\n",
    "    print(f\"Columns: {df_tiktok.columns.tolist()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66492f16",
   "metadata": {},
   "source": [
    "### Prepare data for analysis (normalize text) ###\n",
    "if RUN_TYPE == 1:\n",
    "    # 1. Set Classifier\n",
    "    # Load Hugging Face's emotion classifier\n",
    "    print(\"[Info]\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/bert-base-uncased-emotion\", top_k=None, device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "    # 2. Functions to clean and process text data\n",
    "    # Removing noise from the text\n",
    "    def remove_noise(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"https\\S+|www\\S+httpss\\S+\", '', text, flags=re.MULTILINE) # Remove Url\n",
    "        text = re.sub(r\"\\@w+|\\#\", '', text) # remove @ and #\n",
    "        text = re.sub(r\"[^\\w\\s]\", '', text) # remove punctuation\n",
    "        text_tokens = text.split()\n",
    "        filtered_text = [w for w in text_tokens if not w in stop_words]\n",
    "        return \" \".join(filtered_text)\n",
    "\n",
    "    # Reduction of dimensionality by abstracting word to word stem and truncating text\n",
    "    stemmer = PorterStemmer()\n",
    "    def stem_words(text):\n",
    "        words = text.split()\n",
    "        stemmed_text = [stemmer.stem(word) for word in words]\n",
    "        return stemmed_text\n",
    "\n",
    "    def truncate_text(text, max_length=512):\n",
    "        words = text.split()\n",
    "        return \" \".join(words[:max_length])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca5bca02",
   "metadata": {},
   "source": [
    "### Prepare data for analysis (append emotions) ###\n",
    "if RUN_TYPE == 1:\n",
    "    # 3. Computing and appending emotions to dataframe\n",
    "    def compute_emotions(text):\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            print(\"[ComputeEmotions] Empty cell after data cleaning. Returning 0.0 for all emotions.\")\n",
    "            return {emotion: 0.0 for emotion in ekman_emotions}\n",
    "\n",
    "        try:\n",
    "            # Classify emotions using the Hugging Face pipeline and handle errors\n",
    "            results = classifier(text)[0]\n",
    "            if not results or not isinstance(results, list) or len(results[0]) == 0:\n",
    "                return {emotion: 0.0 for emotion in ekman_emotions}\n",
    "\n",
    "            emotion_scores = {result['label']: result['score'] for result in results}\n",
    "            return {emotion: emotion_scores.get(emotion, 0.0) for emotion in ekman_emotions}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ComputeEmotions] Error while processing text: {text[:20]}... Error: {e}\")\n",
    "            return {emotion: 0.0 for emotion in ekman_emotions}\n",
    "\n",
    "    def append_emotions(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"[AppendEmotions] Column '{text_column}' not found in DataFrame.\")\n",
    "\n",
    "        print(\"[AppendEmotions] Computing emotions for column:\", text_column)\n",
    "\n",
    "        cleaned_column = f\"{text_column}_cleaned\"\n",
    "        df[cleaned_column] = df[text_column].apply(\n",
    "            lambda x: \" \".join(stem_words(remove_noise(x))) if isinstance(x, str) and x.strip() else \"\"\n",
    "        )\n",
    "\n",
    "        # Truncate text if cleaned text exceeds 512 tokens\n",
    "        if (df[cleaned_column].str.split().str.len() > 512).any():\n",
    "            print(\"[AppendEmotions] At least one row with more than 512 tokens - truncating text ...\")\n",
    "            df[cleaned_column] = df[cleaned_column].apply(lambda x: truncate_text(x, max_length=512))\n",
    "\n",
    "        emotion_scores = [compute_emotions(text) for text in tqdm(df[cleaned_column], desc=\"[AppendEmotions] Processing emotions\")]\n",
    "        emotions_df = pd.DataFrame(emotion_scores)\n",
    "        emotions_df.index = df.index\n",
    "        emotions_df.columns = [f\"{text_column}_{emotion}\" for emotion in ekman_emotions]\n",
    "\n",
    "        # Add dominant emotion column\n",
    "        dominant = emotions_df.idxmax(axis=1).apply(lambda x: x.split('_')[-1])\n",
    "        all_zero = emotions_df.eq(0.0).all(axis=1)\n",
    "        dominant[all_zero] = np.nan\n",
    "        emotions_df[f\"{text_column}_dominant_emotion\"] = dominant\n",
    "\n",
    "        # Insert right hand of input text_column\n",
    "        insert_at = df.columns.get_loc(text_column) + 1\n",
    "\n",
    "        # DataFrame in drei Teile splitten und zusammenfügen\n",
    "        left = df.iloc[:, :insert_at]\n",
    "        right = df.iloc[:, insert_at:].drop(columns=[cleaned_column], errors='ignore')\n",
    "        result_df = pd.concat([left, df[[cleaned_column]], emotions_df, right], axis=1)\n",
    "\n",
    "        return result_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4379b21",
   "metadata": {},
   "source": [
    "### Create/Read dataset: Performing emotion analysis ###\n",
    "if RUN_TYPE == 1:\n",
    "    # Append emotions to TikTok DataFrame and safe to file\n",
    "    df_tiktok = append_emotions(df_tiktok, 'combined_text')\n",
    "    df_tiktok.to_csv('../data/tiktok/tiktok_emotions.csv', index=False)\n",
    "\n",
    "df_tiktok_emotions = pd.read_csv('../data/tiktok/tiktok_emotions.csv')\n",
    "\n",
    "# Count dominant emotions\n",
    "emotion_counts = df_tiktok_emotions['combined_text_dominant_emotion'].value_counts()\n",
    "\n",
    "# Plot emotions in barchart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(emotion_counts.index, emotion_counts.values)\n",
    "plt.title('Anzahl der Emotionen in TikTok Videos')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Anzahl')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Zeige die Zahlen\n",
    "print(f\"Anzahl der Videos: {len(df_tiktok_emotions)}\")\n",
    "print(f\"Anzahl Emotionen in Videos:\\n{emotion_counts}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "195ae062",
   "metadata": {},
   "source": [
    "<h1>TikTok Analysis - Part 2</h1>\n",
    "<h2><i>Event Study</i></h2>\n",
    "<h3>[2.1][Data Preparation]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "id": "ebb4e75b",
   "metadata": {},
   "source": [
    "### Read necessary data for event study (tiktok, us_stock_data, ger_stock_data) ###\n",
    "if RUN_TYPE == 1:\n",
    "    # TikTok data\n",
    "    # see above\n",
    "\n",
    "    # Stock data (US)\n",
    "    df_us_stock_data = pd.read_csv('../legacy/data/tsla_intraday_202305_202504-1m.csv')\n",
    "    df_us_stock_data = df_us_stock_data.rename(\n",
    "        columns={'Unnamed: 0': 'timestamp'}\n",
    "    )\n",
    "    df_us_stock_data['timestamp'] = pd.to_datetime(\n",
    "        df_us_stock_data['timestamp']\n",
    "    ).dt.tz_localize(\n",
    "        tz=eastern\n",
    "    )\n",
    "    df_us_stock_data['timestamp'].copy().sort_values(ascending=True, inplace=True)\n",
    "    df_us_stock_data.set_index('timestamp').sort_index()\n",
    "\n",
    "    # Stock data (GER)\n",
    "    df_ger_stock_data = pd.read_csv('../legacy/data/TSL0_intraday_230501_250501-1m.csv').drop(columns=['Unnamed: 0'])\n",
    "    df_ger_stock_data = df_ger_stock_data.rename(\n",
    "        columns={'datetime': 'timestamp'}\n",
    "    )\n",
    "\n",
    "    df_ger_stock_data['timestamp'] = pd.to_datetime(\n",
    "        df_ger_stock_data['timestamp']\n",
    "    ).dt.tz_localize(\n",
    "        tz=european\n",
    "    )\n",
    "    df_ger_stock_data.set_index('timestamp').sort_index()\n",
    "\n",
    "    # Save to file\n",
    "    df_tiktok_emotions.to_csv('../data/tiktok/tiktok_emotions.csv', index=False)\n",
    "\n",
    "    df_ger_stock_data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ee6d570a",
   "metadata": {},
   "source": [
    "<h3>[2.2][Log Transformation]</h3>\n",
    "<p>In the following, similarly to the twitter data analysis, we compute the log_return and log_volume for the fin data. Since in the tsl0_intraday data file we have some points in time without data, we also need to fill it. For this, we use the foreward fill ffill() method for the return and set the volume to 0 for the new tuple.</p>"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ea3cb89",
   "metadata": {},
   "source": [
    "### Foreward fill missing values in stock data ###\n",
    "if RUN_TYPE == 1:\n",
    "    def fill_missing_timestamps(df):\n",
    "        df = df.copy()\n",
    "\n",
    "        if df.index.name != 'timestamp':\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "        start_time = df.index.min()\n",
    "        end_time = df.index.max()\n",
    "\n",
    "        # Filter trading hours\n",
    "        full_index = pd.date_range(\n",
    "            start=start_time.replace(hour=4, minute=0, second=0, microsecond=0),\n",
    "            end=end_time.replace(hour=22, minute=59, second=0, microsecond=0),\n",
    "            freq='1min'\n",
    "        )\n",
    "\n",
    "        # Filter business days\n",
    "        business_minutes = full_index[full_index.dayofweek < 5]\n",
    "\n",
    "        # Reindex and fill data\n",
    "        df_filled = df.reindex(business_minutes)\n",
    "\n",
    "        # Forward fill prices\n",
    "        price_cols = ['open', 'high', 'low', 'close']\n",
    "        df_filled[price_cols] = df_filled[price_cols].ffill()\n",
    "\n",
    "        # Fill Volume with 0\n",
    "        df_filled['volume'] = df_filled['volume'].fillna(0)\n",
    "\n",
    "        return df_filled\n",
    "\n",
    "    def compute_stock_measures(df, last_date='2025-04-30'):\n",
    "\n",
    "        # Check if DataFrame is already processed\n",
    "        required_columns = [\n",
    "            'log_return', 'log_return_z', 'log_return_z_intraday',\n",
    "            'log_volume', 'log_volume_z', 'log_volume_z_intraday',\n",
    "            'minute_of_day'\n",
    "        ]\n",
    "\n",
    "        if all(col in df.columns for col in required_columns):\n",
    "            print(\"[Info] Daten bereits vollständig verarbeitet\")\n",
    "\n",
    "            # Check date\n",
    "            cutoff_timestamp = pd.Timestamp(f'{last_date} 22:59:00', tz=df.index.tz)\n",
    "            if df.index.max() > cutoff_timestamp:\n",
    "                print(f\"[Info] Schneide Daten nach {last_date} ab\")\n",
    "                df = df[df.index <= cutoff_timestamp]\n",
    "\n",
    "            return df\n",
    "\n",
    "        # Fill missing timestamps\n",
    "        df = fill_missing_timestamps(df)\n",
    "\n",
    "        # Filter data until last_date\n",
    "        cutoff_timestamp = pd.Timestamp(f'{last_date} 22:59:00', tz=df.index.tz)\n",
    "        df = df[df.index <= cutoff_timestamp]\n",
    "\n",
    "        # Compute missing column values\n",
    "        if 'minute_of_day' not in df.columns:\n",
    "            df['minute_of_day'] = df.index.hour * 60 + df.index.minute\n",
    "\n",
    "        if 'log_return' not in df.columns:\n",
    "            df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "            df['log_return_z'] = (df['log_return'] - df['log_return'].mean()) / df['log_return'].std()\n",
    "            df['log_return_z_intraday'] = df.groupby('minute_of_day')['log_return'].transform(\n",
    "                lambda x: (x - x.mean()) / x.std()\n",
    "            )\n",
    "\n",
    "        if 'log_volume' not in df.columns:\n",
    "            df['log_volume'] = np.log1p(df['volume'])\n",
    "            df['log_volume_z'] = (df['log_volume'] - df['log_volume'].mean()) / df['log_volume'].std()\n",
    "            df['log_volume_z_intraday'] = df.groupby('minute_of_day')['log_volume'].transform(\n",
    "                lambda x: (x - x.mean()) / x.std()\n",
    "            )\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6c82cfeae0a4",
   "metadata": {},
   "source": [
    "### Compute stock measures (log_return/volume) for US and GER stock data + forward fill fin data ###\n",
    "# Create new csv files\n",
    "if RUN_TYPE == 1:\n",
    "    df_us_stock_data = compute_stock_measures(df_us_stock_data)\n",
    "    df_us_stock_data.to_csv(\n",
    "        path_or_buf='../data/stocks/tesla_nyse_intraday_202305_202504-1m.csv',\n",
    "        index=True\n",
    "    )\n",
    "\n",
    "    df_ger_stock_data = compute_stock_measures(df_ger_stock_data)\n",
    "    df_ger_stock_data.to_csv(\n",
    "        path_or_buf='../data/stocks/tesla_xetra_intraday_202305_202504-1m.csv',\n",
    "        index=True\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
