{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be002d53",
   "metadata": {},
   "source": [
    "<h1>TikTok Analysis - Part 1</h1>\n",
    "<h2><i>Data Preparation</i></h2>"
   ]
  },
  {
   "cell_type": "code",
   "id": "7cbe182e",
   "metadata": {},
   "source": [
    "### Imports ###\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from src.TimeNormalizer import TimeNormalizer\n",
    "\n",
    "### Set necessary workspace variables ###\n",
    "\n",
    "# Set execution type (to avoid repeating resource intensive operations)\n",
    "#RUN_TYPE = 0 # set to 0 to avoid file creation process\n",
    "RUN_TYPE = 1 # set to 1 to perform emotion analysis file creation process\n",
    "#RUN_TYPE = 2 # set to 2 to perform topic analysis file creation process\n",
    "\n",
    "# Define Ekman's emotions \n",
    "ekman_emotions = ['anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "\n",
    "# Timezones\n",
    "eastern = pytz.timezone(\"US/Eastern\")\n",
    "european = pytz.timezone(\"Europe/Berlin\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec062195",
   "metadata": {},
   "source": [
    "### Read and transform data (total_engagement, combined_text, set_timezone) ###\n",
    "# Read dataframe\n",
    "if RUN_TYPE == 1:\n",
    "    df_tiktok = pd.read_excel('../data/tiktok/tiktok_transcript.xlsx').drop(columns=['Hashtag', 'URL', 'Author']).dropna()\n",
    "\n",
    "    # Extract total_engagement and combine text columns\n",
    "    df_tiktok['total_engagement'] = df_tiktok['Likes'] + df_tiktok['Comments'] + df_tiktok['Shares']\n",
    "    df_tiktok['combined_text'] = df_tiktok['CaptionCleaned'].fillna('') + ' ' + df_tiktok['Transcript'].fillna('')\n",
    "    df_tiktok.drop(columns=['Likes', 'Comments', 'Shares', 'Caption', 'CaptionCleaned', 'Transcript'], inplace=True)\n",
    "\n",
    "    # Convert 'Date' column to datetime format and set timezone\n",
    "    df_tiktok = df_tiktok.rename(columns={'Created': 'timestamp'})\n",
    "    df_tiktok['timestamp'] = pd.to_datetime(\n",
    "        df_tiktok['timestamp'],\n",
    "        format=\"%a %b %d %H:%M:%S %z %Y\",\n",
    "        errors='coerce'\n",
    "    )\n",
    "    df_tiktok['timestamp'] = df_tiktok['timestamp'].dt.tz_localize(eastern)\n",
    "\n",
    "    print(f\"Länge df: {len(df_tiktok)}\")\n",
    "    print(f\"Columns: {df_tiktok.columns.tolist()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66492f16",
   "metadata": {},
   "source": [
    "### Prepare data for analysis (normalize text) ###\n",
    "if RUN_TYPE == 1:\n",
    "    # 1. Set Classifier\n",
    "    # Load Hugging Face's emotion classifier\n",
    "    print(\"[Info]\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/bert-base-uncased-emotion\", top_k=None, device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "    # 2. Functions to clean and process text data\n",
    "    # Removing noise from the text\n",
    "    def remove_noise(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"https\\S+|www\\S+httpss\\S+\", '', text, flags=re.MULTILINE) # Remove Url\n",
    "        text = re.sub(r\"\\@w+|\\#\", '', text) # remove @ and #\n",
    "        text = re.sub(r\"[^\\w\\s]\", '', text) # remove punctuation\n",
    "        text_tokens = text.split()\n",
    "        filtered_text = [w for w in text_tokens if not w in stop_words]\n",
    "        return \" \".join(filtered_text)\n",
    "\n",
    "    # Reduction of dimensionality by abstracting word to word stem and truncating text\n",
    "    stemmer = PorterStemmer()\n",
    "    def stem_words(text):\n",
    "        words = text.split()\n",
    "        stemmed_text = [stemmer.stem(word) for word in words]\n",
    "        return stemmed_text\n",
    "\n",
    "    def truncate_text(text, max_length=512):\n",
    "        words = text.split()\n",
    "        return \" \".join(words[:max_length])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca5bca02",
   "metadata": {},
   "source": [
    "### Prepare data for analysis (append emotions) ###\n",
    "if RUN_TYPE == 1:\n",
    "    # 3. Computing and appending emotions to dataframe\n",
    "    def compute_emotions(text):\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            print(\"[ComputeEmotions] Empty cell after data cleaning. Returning 0.0 for all emotions.\")\n",
    "            return {emotion: 0.0 for emotion in ekman_emotions}\n",
    "\n",
    "        try:\n",
    "            # Classify emotions using the Hugging Face pipeline and handle errors\n",
    "            results = classifier(text)[0]\n",
    "            if not results or not isinstance(results, list) or len(results[0]) == 0:\n",
    "                return {emotion: 0.0 for emotion in ekman_emotions}\n",
    "\n",
    "            emotion_scores = {result['label']: result['score'] for result in results}\n",
    "            return {emotion: emotion_scores.get(emotion, 0.0) for emotion in ekman_emotions}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ComputeEmotions] Error while processing text: {text[:20]}... Error: {e}\")\n",
    "            return {emotion: 0.0 for emotion in ekman_emotions}\n",
    "\n",
    "    def append_emotions(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"[AppendEmotions] Column '{text_column}' not found in DataFrame.\")\n",
    "\n",
    "        print(\"[AppendEmotions] Computing emotions for column:\", text_column)\n",
    "\n",
    "        cleaned_column = f\"{text_column}_cleaned\"\n",
    "        df[cleaned_column] = df[text_column].apply(\n",
    "            lambda x: \" \".join(stem_words(remove_noise(x))) if isinstance(x, str) and x.strip() else \"\"\n",
    "        )\n",
    "\n",
    "        # Truncate text if cleaned text exceeds 512 tokens\n",
    "        if (df[cleaned_column].str.split().str.len() > 512).any():\n",
    "            print(\"[AppendEmotions] At least one row with more than 512 tokens - truncating text ...\")\n",
    "            df[cleaned_column] = df[cleaned_column].apply(lambda x: truncate_text(x, max_length=512))\n",
    "\n",
    "        emotion_scores = [compute_emotions(text) for text in tqdm(df[cleaned_column], desc=\"[AppendEmotions] Processing emotions\")]\n",
    "        emotions_df = pd.DataFrame(emotion_scores)\n",
    "        emotions_df.index = df.index\n",
    "        emotions_df.columns = [f\"{text_column}_{emotion}\" for emotion in ekman_emotions]\n",
    "\n",
    "        # Add dominant emotion column\n",
    "        dominant = emotions_df.idxmax(axis=1).apply(lambda x: x.split('_')[-1])\n",
    "        all_zero = emotions_df.eq(0.0).all(axis=1)\n",
    "        dominant[all_zero] = np.nan\n",
    "        emotions_df[f\"{text_column}_dominant_emotion\"] = dominant\n",
    "\n",
    "        # Insert right hand of input text_column\n",
    "        insert_at = df.columns.get_loc(text_column) + 1\n",
    "\n",
    "        # DataFrame in drei Teile splitten und zusammenfügen\n",
    "        left = df.iloc[:, :insert_at]\n",
    "        right = df.iloc[:, insert_at:].drop(columns=[cleaned_column], errors='ignore')\n",
    "        result_df = pd.concat([left, df[[cleaned_column]], emotions_df, right], axis=1)\n",
    "\n",
    "        return result_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4379b21",
   "metadata": {},
   "source": [
    "### Create/Read dataset: Performing emotion analysis ###\n",
    "if RUN_TYPE == 1:\n",
    "    # Append emotions to TikTok DataFrame and safe to file\n",
    "    df_tiktok = append_emotions(df_tiktok, 'combined_text')\n",
    "    df_tiktok.to_csv('../data/tiktok/tiktok_emotions.csv', index=False)\n",
    "\n",
    "df_tiktok_emotions = pd.read_csv('../data/tiktok/tiktok_emotions.csv')\n",
    "\n",
    "# Count dominant emotions\n",
    "emotion_counts = df_tiktok_emotions['combined_text_dominant_emotion'].value_counts()\n",
    "\n",
    "# Plot emotions in barchart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(emotion_counts.index, emotion_counts.values)\n",
    "plt.title('Anzahl der Emotionen in TikTok Videos')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Anzahl')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Zeige die Zahlen\n",
    "print(f\"Anzahl der Videos: {len(df_tiktok_emotions)}\")\n",
    "print(f\"Anzahl Emotionen in Videos:\\n{emotion_counts}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "195ae062",
   "metadata": {},
   "source": [
    "<h1>TikTok Analysis - Part 2</h1>\n",
    "<h2><i>Event Study</i></h2>\n",
    "<h3>[2.1][Data Preparation]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "id": "ebb4e75b",
   "metadata": {},
   "source": [
    "### Read necessary data for event study (tiktok, us_stock_data, ger_stock_data) ###\n",
    "if RUN_TYPE == 1:\n",
    "    # TikTok data\n",
    "    # see above\n",
    "\n",
    "    # Stock data (US)\n",
    "    df_us_stock_data = pd.read_csv('../legacy/data/tsla_intraday_202305_202504-1m.csv')\n",
    "    df_us_stock_data = df_us_stock_data.rename(\n",
    "        columns={'Unnamed: 0': 'timestamp'}\n",
    "    )\n",
    "    df_us_stock_data['timestamp'] = pd.to_datetime(\n",
    "        df_us_stock_data['timestamp']\n",
    "    ).dt.tz_localize(\n",
    "        tz=eastern\n",
    "    )\n",
    "    df_us_stock_data['timestamp'].copy().sort_values(ascending=True, inplace=True)\n",
    "    df_us_stock_data.set_index('timestamp').sort_index()\n",
    "\n",
    "    # Stock data (GER)\n",
    "    df_ger_stock_data = pd.read_csv('../legacy/data/TSL0_intraday_230501_250501-1m.csv').drop(columns=['Unnamed: 0'])\n",
    "    df_ger_stock_data = df_ger_stock_data.rename(\n",
    "        columns={'datetime': 'timestamp'}\n",
    "    )\n",
    "\n",
    "    df_ger_stock_data['timestamp'] = pd.to_datetime(\n",
    "        df_ger_stock_data['timestamp']\n",
    "    ).dt.tz_localize(\n",
    "        tz=european\n",
    "    )\n",
    "    df_ger_stock_data.set_index('timestamp').sort_index()\n",
    "\n",
    "    # Save to file\n",
    "    df_tiktok_emotions.to_csv('../data/tiktok/tiktok_emotions.csv', index=False)\n",
    "\n",
    "    df_ger_stock_data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ee6d570a",
   "metadata": {},
   "source": [
    "<h3>[2.2][Log Transformation]</h3>\n",
    "<p>In the following, similarly to the twitter data analysis, we compute the log_return and log_volume for the fin data. Since in the tsl0_intraday data file we have some points in time without data, we also need to fill it. For this, we use the foreward fill ffill() method for the return and set the volume to 0 for the new tuple.</p>"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ea3cb89",
   "metadata": {},
   "source": [
    "### Foreward fill missing values in stock data ###\n",
    "if RUN_TYPE == 1:\n",
    "    def fill_missing_timestamps(df):\n",
    "        df = df.copy()\n",
    "\n",
    "        if df.index.name != 'timestamp':\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "        start_time = df.index.min()\n",
    "        end_time = df.index.max()\n",
    "\n",
    "        # Filter trading hours\n",
    "        full_index = pd.date_range(\n",
    "            start=start_time.replace(hour=4, minute=0, second=0, microsecond=0),\n",
    "            end=end_time.replace(hour=22, minute=59, second=0, microsecond=0),\n",
    "            freq='1min'\n",
    "        )\n",
    "\n",
    "        # Filter business days\n",
    "        business_minutes = full_index[full_index.dayofweek < 5]\n",
    "\n",
    "        # Reindex and fill data\n",
    "        df_filled = df.reindex(business_minutes)\n",
    "\n",
    "        # Forward fill prices\n",
    "        price_cols = ['open', 'high', 'low', 'close']\n",
    "        df_filled[price_cols] = df_filled[price_cols].ffill()\n",
    "\n",
    "        # Fill Volume with 0\n",
    "        df_filled['volume'] = df_filled['volume'].fillna(0)\n",
    "\n",
    "        return df_filled\n",
    "\n",
    "    def compute_stock_measures(df, last_date='2025-04-30'):\n",
    "\n",
    "        # Check if DataFrame is already processed\n",
    "        required_columns = [\n",
    "            'log_return', 'log_return_z', 'log_return_z_intraday',\n",
    "            'log_volume', 'log_volume_z', 'log_volume_z_intraday',\n",
    "            'minute_of_day'\n",
    "        ]\n",
    "\n",
    "        if all(col in df.columns for col in required_columns):\n",
    "            print(\"[Info] Daten bereits vollständig verarbeitet\")\n",
    "\n",
    "            # Check date\n",
    "            cutoff_timestamp = pd.Timestamp(f'{last_date} 22:59:00', tz=df.index.tz)\n",
    "            if df.index.max() > cutoff_timestamp:\n",
    "                print(f\"[Info] Schneide Daten nach {last_date} ab\")\n",
    "                df = df[df.index <= cutoff_timestamp]\n",
    "\n",
    "            return df\n",
    "\n",
    "        # Fill missing timestamps\n",
    "        df = fill_missing_timestamps(df)\n",
    "\n",
    "        # Filter data until last_date\n",
    "        cutoff_timestamp = pd.Timestamp(f'{last_date} 22:59:00', tz=df.index.tz)\n",
    "        df = df[df.index <= cutoff_timestamp]\n",
    "\n",
    "        # Compute missing column values\n",
    "        if 'minute_of_day' not in df.columns:\n",
    "            df['minute_of_day'] = df.index.hour * 60 + df.index.minute\n",
    "\n",
    "        if 'log_return' not in df.columns:\n",
    "            df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "            df['log_return_z'] = (df['log_return'] - df['log_return'].mean()) / df['log_return'].std()\n",
    "            df['log_return_z_intraday'] = df.groupby('minute_of_day')['log_return'].transform(\n",
    "                lambda x: (x - x.mean()) / x.std()\n",
    "            )\n",
    "\n",
    "        if 'log_volume' not in df.columns:\n",
    "            df['log_volume'] = np.log1p(df['volume'])\n",
    "            df['log_volume_z'] = (df['log_volume'] - df['log_volume'].mean()) / df['log_volume'].std()\n",
    "            df['log_volume_z_intraday'] = df.groupby('minute_of_day')['log_volume'].transform(\n",
    "                lambda x: (x - x.mean()) / x.std()\n",
    "            )\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6c82cfeae0a4",
   "metadata": {},
   "source": [
    "### Compute stock measures (log_return/volume) for US and GER stock data + forward fill fin data ###\n",
    "# Create new csv files\n",
    "if RUN_TYPE == 1:\n",
    "    df_us_stock_data = compute_stock_measures(df_us_stock_data)\n",
    "    df_us_stock_data.to_csv(\n",
    "        path_or_buf='../data/stocks/tesla_nyse_intraday_202305_202504-1m.csv',\n",
    "        index=True\n",
    "    )\n",
    "\n",
    "    df_ger_stock_data = compute_stock_measures(df_ger_stock_data)\n",
    "    df_ger_stock_data.to_csv(\n",
    "        path_or_buf='../data/stocks/tesla_xetra_intraday_202305_202504-1m.csv',\n",
    "        index=True\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>[2.2][Create Event Study Dataframe]</h3>",
   "id": "1c0af43fae21f741"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T21:07:17.502368Z",
     "start_time": "2025-07-15T21:07:09.787068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Read necessary dataframes, set index, and convert timezones ###\n",
    "# Prep tweets\n",
    "df_tesla = pd.read_csv('../data/tiktok/tiktok_emotions.csv').dropna()\n",
    "df_tesla = df_tesla.drop(columns={'combined_text', 'combined_text_cleaned'}).rename(columns={\n",
    "    'total_engagement': 'engagement',\n",
    "    'combined_text_anger': 'anger',\n",
    "    'combined_text_fear': 'fear',\n",
    "    'combined_text_joy': 'joy',\n",
    "    'combined_text_sadness': 'sadness',\n",
    "    'combined_text_disgust': 'disgust',\n",
    "    'combined_text_surprise': 'surprise',\n",
    "    'combined_text_dominant_emotion': 'video_emotion'\n",
    "})\n",
    "df_tesla['timestamp'] = pd.to_datetime(df_tesla['timestamp'], format='%Y-%m-%d %H:%M:%S%z', utc=True)\n",
    "# Prep nyse data\n",
    "df_nyse = (pd.read_csv('../data/stocks/tesla_nyse_intraday_202305_202504-1m.csv')\n",
    "    .rename(columns={'Unnamed: 0': 'timestamp'})\n",
    "    .drop(columns={'open', 'high', 'low', 'close', 'volume'})\n",
    ").set_index('timestamp').sort_index()\n",
    "\n",
    "# Prep xetra data\n",
    "df_xetra = (pd.read_csv('../data/stocks/tesla_xetra_intraday_202305_202504-1m.csv')\n",
    "    .rename(columns={'Unnamed: 0': 'timestamp'})\n",
    "    .drop(columns={'open', 'high', 'low', 'close', 'volume'})\n",
    ").set_index('timestamp').sort_index()"
   ],
   "id": "6d4ac47b",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T21:05:40.329113Z",
     "start_time": "2025-07-15T21:05:40.323525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "#df_tesla['timestamp'] = pd.to_datetime(df_tesla['timestamp'], format='%Y-%m-%d %H:%M:%S%z', utc=True)\n",
    "print(\"Datentyp der Spalte:\", df_tesla['timestamp'].dtype)\n",
    "print(\"Beispielwerte:\", df_tesla['timestamp'].head(3).to_list())\n",
    "print(\"Zeitzoneninfo:\", df_tesla['timestamp'].dt.tz)\n",
    "print(\"Tesla Columns:\" , df_tesla.columns)"
   ],
   "id": "839db13b431c0dbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datentyp der Spalte: datetime64[ns, UTC]\n",
      "Beispielwerte: [Timestamp('2024-05-24 04:03:33+0000', tz='UTC'), Timestamp('2024-10-10 19:21:19+0000', tz='UTC'), Timestamp('2025-04-08 21:05:48+0000', tz='UTC')]\n",
      "Zeitzoneninfo: UTC\n",
      "Tesla Columns: Index(['timestamp', 'engagement', 'anger', 'fear', 'joy', 'sadness', 'disgust',\n",
      "       'surprise', 'tweet_emotion'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T21:07:20.382551Z",
     "start_time": "2025-07-15T21:07:20.249459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Group tweets to events with dominant emotion ###\n",
    "\n",
    "## Pre-process tweets for event study\n",
    "# 1. Filter out tweets outside market hours (+-2h)\n",
    "df_tesla = df_tesla[\n",
    "    df_tesla['timestamp'].dt.time >= pd.to_datetime(\"04:00\").time()\n",
    "]\n",
    "df_tesla = df_tesla[\n",
    "    df_tesla['timestamp'].dt.time <= pd.to_datetime(\"17:59\").time()\n",
    "]\n",
    "\n",
    "# 2. Sort by time\n",
    "df_tesla = df_tesla.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# 3. Compute time difference between tweets\n",
    "df_tesla['time_diff'] = df_tesla['timestamp'].diff()\n",
    "\n",
    "# 4. Cluster Tweets to events with time_diff >= 7h\n",
    "df_tesla['new_event'] = df_tesla['time_diff'] > pd.Timedelta(hours=7)\n",
    "\n",
    "# 5. Cumulate events in event_id\n",
    "df_tesla['event_id'] = df_tesla['new_event'].cumsum()\n",
    "\n",
    "# 6. Compute the dominant emotion for each event and merge back to df\n",
    "def compute_event_emotion(group):\n",
    "    total_engagement = group['engagement'].sum()\n",
    "    weighted_emo_scores = {\n",
    "        emotion: (group[emotion] * group['engagement']).sum() / total_engagement\n",
    "        for emotion in ekman_emotions\n",
    "    }\n",
    "    event_emotion = max(weighted_emo_scores, key=weighted_emo_scores.get)\n",
    "    return pd.Series({\n",
    "        **weighted_emo_scores,\n",
    "        'event_emotion': event_emotion\n",
    "    })\n",
    "\n",
    "if not ('event_emotion' in df_tesla.columns):\n",
    "    df_event_emotions = (df_tesla\n",
    "        .drop(columns=['event_id'])\n",
    "        .groupby(df_tesla['event_id'], group_keys=False)\n",
    "        .apply(compute_event_emotion)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    df_tesla = (df_tesla\n",
    "        .merge(\n",
    "            df_event_emotions[['event_id', 'event_emotion']],\n",
    "            on='event_id',\n",
    "            how='left'\n",
    "        )\n",
    "        .drop(columns={'anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise', 'video_emotion', 'new_event'})\n",
    "    )\n",
    "df_tesla.head()\n",
    "\n",
    "# 8. Set event from time to time of the earliest tweet (models from what point in time a reaction can occur)\n",
    "df_tesla['event_time'] = df_tesla.groupby('event_id')['timestamp'].transform('min')\n",
    "\n",
    "# 9. Filter duplicate events\n",
    "df_tesla = df_tesla.drop_duplicates(subset=['event_id'])\n",
    "\n",
    "print(\"[Info] Number of unique events = \", df_tesla['event_id'].nunique(), \"\\n\")"
   ],
   "id": "16fd82a7b095806c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Number of unique events =  114 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T21:25:58.871503Z",
     "start_time": "2025-07-15T21:25:58.867811Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Index Type:\", df_nyse.index.dtype)",
   "id": "b22de51b81e8c0ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Type: object\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T21:39:39.741057Z",
     "start_time": "2025-07-15T21:39:25.012209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Create/Read Event Study DataFrame (with actual, expected, abnormal return and volume) ###\n",
    "\n",
    "# 1. Filter events based on overlaps, estimation and observation windows\n",
    "# 2. Calculate expected and abnormal returns for each event\n",
    "def filter_events(df, df_trades, get_pre = 0):\n",
    "    # Define necessary vars\n",
    "    est = 300\n",
    "    obs = 120\n",
    "    gap = est + obs\n",
    "    results = []\n",
    "\n",
    "    # 1. Filter by Overlaps\n",
    "    valid_events = []\n",
    "    last_event = None\n",
    "\n",
    "    # Add valid events to the list\n",
    "    for date in df.index:\n",
    "        # First event always valid\n",
    "        if last_event is None:\n",
    "            valid_events.append(date)\n",
    "            last_event = date\n",
    "        else:\n",
    "            # If timeshift > gaps, keep the current event and set as new last_event\n",
    "            if (date - last_event) > pd.Timedelta(minutes=gap):\n",
    "                valid_events.append(date)\n",
    "                last_event = date\n",
    "\n",
    "    # Only keep valid events\n",
    "    df = df.loc[valid_events]\n",
    "    print(f\"[Overlap] New df_length = {len(df)}\")\n",
    "\n",
    "    # 2. Align event_time with the next available trading timestamp\n",
    "    df.index = df.index.tz_convert(eastern)\n",
    "\n",
    "    for i, date in enumerate(df.index):\n",
    "        if date not in df_trades.index:\n",
    "            future_times = df_trades.index[df_trades.index > date]\n",
    "            if not future_times.empty:\n",
    "                df.index.values[i] = pd.Timestamp(future_times[0])\n",
    "\n",
    "    # 3. Filter events without sufficient estimation_window data\n",
    "    # 4. Filter events without sufficient event_window data\n",
    "    valid_events = []\n",
    "\n",
    "    for date in df.index:\n",
    "        day_data = df_trades[df_trades.index.date == date.date()]\n",
    "        event_index = day_data.index.get_loc(date)\n",
    "\n",
    "        remaining = len(day_data) - event_index\n",
    "\n",
    "        if event_index >= est:\n",
    "            if remaining >= obs:\n",
    "                valid_events.append(date)\n",
    "\n",
    "    df = df.loc[valid_events]\n",
    "    print(f\"[Sum Data] New df_length = {len(df)}\")\n",
    "\n",
    "    # 5. Event study variables\n",
    "    for event_row in df.itertuples():\n",
    "        event_id = event_row.event_id\n",
    "        event_emo = event_row.event_emotion\n",
    "        event_time = event_row.Index\n",
    "\n",
    "        estimation_window = df_trades.loc[\n",
    "            event_time - pd.Timedelta(minutes=est) : event_time - pd.Timedelta(minutes=1)\n",
    "        ]\n",
    "        event_window = df_trades.loc[\n",
    "            event_time - pd.Timedelta(minutes=get_pre * est): event_time + pd.Timedelta(minutes=obs - 1)\n",
    "        ]\n",
    "\n",
    "        expected_return = estimation_window['log_return'].mean()\n",
    "        expected_volume = estimation_window['log_volume'].median() # robust against outliers, see Bamber (1987)\n",
    "        expected_volume_z_intraday = estimation_window['log_volume_z_intraday'].median()\n",
    "\n",
    "        for i, fin_row in enumerate(event_window.itertuples()):\n",
    "            if get_pre == 1:\n",
    "                i = int((fin_row.Index - event_time).total_seconds() // 60)\n",
    "\n",
    "            actual_return = fin_row.log_return\n",
    "            abnormal_return = actual_return - expected_return\n",
    "\n",
    "            actual_volume = fin_row.log_volume\n",
    "            abnormal_volume = actual_volume - expected_volume\n",
    "\n",
    "            actual_volume_z_intraday = fin_row.log_volume_z_intraday\n",
    "            abnormal_volume_z_intraday = actual_volume_z_intraday - expected_volume_z_intraday\n",
    "\n",
    "            results.append({\n",
    "                'event_id': event_id,\n",
    "                'event_fin_offset': i,\n",
    "                'event_time': event_time,\n",
    "                'fin_time': fin_row.Index,\n",
    "                'event_emo': event_emo,\n",
    "                'actual_return': actual_return,\n",
    "                'expected_return': expected_return,\n",
    "                'abnormal_return': abnormal_return,\n",
    "                'actual_volume': actual_volume,\n",
    "                'expected_volume': expected_volume,\n",
    "                'abnormal_volume': abnormal_volume,\n",
    "                'actual_volume_z_intraday': actual_volume_z_intraday,\n",
    "                'expected_volume_z_intraday': expected_volume_z_intraday,\n",
    "                'abnormal_volume_z_intraday': abnormal_volume_z_intraday\n",
    "            })\n",
    "\n",
    "    # Make a dataframe from the result list\n",
    "    df_valid_events = pd.DataFrame(results)\n",
    "\n",
    "    return df_valid_events\n",
    "\n",
    "if RUN_TYPE == 1: # if not yet created, create the event study dataframes with abnormal returns and volumes\n",
    "\n",
    "    #  Create new dfs with necessary event data and set time column as index\n",
    "    df_events = (df_tesla[['event_id', 'event_emotion', 'event_time']]\n",
    "                .set_index('event_time')\n",
    "                .sort_index())\n",
    "\n",
    "    df_nyse = df_nyse[['log_return', 'log_volume', 'log_volume_z_intraday']] # keeps index col ('timestamp')\n",
    "    df_nyse.index = pd.to_datetime(\n",
    "        df_nyse.index,\n",
    "        format='%Y-%m-%d %H:%M:%S%z',\n",
    "        utc=True,\n",
    "        errors='coerce'\n",
    "    )\n",
    "    df_nyse = df_nyse.sort_index()\n",
    "\n",
    "    df_xetra = df_xetra[['log_return', 'log_volume', 'log_volume_z_intraday']] # keeps index col ('timestamp')\n",
    "    df_xetra.index = pd.to_datetime(\n",
    "        df_xetra.index,\n",
    "        format='%Y-%m-%d %H:%M:%S%z',\n",
    "        utc=True,\n",
    "        errors='coerce'\n",
    "    )\n",
    "    df_xetra = df_xetra.sort_index()\n",
    "\n",
    "    for df_trades, market in zip([df_nyse, df_xetra], ['nyse', 'xetra']):\n",
    "        df_event_study = filter_events(df_events, df_trades, get_pre = 0).to_csv(\n",
    "            f'../data/twitter/tiktok_{market}_emotion_event_study.csv',\n",
    "            index=False\n",
    "        )\n",
    "        df_event_study_pre_post = filter_events(df_events, df_trades, get_pre=1).to_csv(\n",
    "            f'../data/twitter/tiktok_{market}emotion_event_study_pre_post.csv',\n",
    "            index=False)\n",
    "\n",
    "print(\"[Info] Dataframe Inspection\")\n",
    "print(f\"Emotions: {df_event_study['event_emo'].unique()}\")\n",
    "emotion = df_event_study['event_emo'].unique()\n",
    "for emo in emotion:\n",
    "    print(f\"  - {emo}: {len(df_event_study[df_event_study['event_emo'] == emo].groupby('event_id'))} events\")\n",
    "\n",
    "print(f\"\\n[Info] Event Study DataFrame has {len(df_event_study)} entries.\")\n",
    "print(f\"[Info] Event Study Pre/Post DataFrame has {len(df_event_study_pre_post)} entries.\\n\")\n",
    "if (df_event_study_pre_post.columns == df_event_study.columns).all():\n",
    "    print(\"[Info] emotion_event_study(_pre_post).csv files have the following columns:\")\n",
    "    print(df_event_study.columns.tolist())"
   ],
   "id": "4a8bf09da1e8eeb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Overlap] New df_length = 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aklei\\AppData\\Local\\Temp\\ipykernel_92172\\274416320.py:39: UserWarning: no explicit representation of timezones available for np.datetime64\n",
      "  df.index.values[i] = pd.Timestamp(future_times[0])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Timestamp('2023-11-30 23:06:00-0500', tz='US/Eastern')",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mindex.pyx:609\u001B[39m, in \u001B[36mpandas._libs.index.DatetimeEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2606\u001B[39m, in \u001B[36mpandas._libs.hashtable.Int64HashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2630\u001B[39m, in \u001B[36mpandas._libs.hashtable.Int64HashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mKeyError\u001B[39m: 1701403560000000000",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\coin\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3804\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3805\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._engine.get_loc(casted_key)\n\u001B[32m   3806\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mindex.pyx:577\u001B[39m, in \u001B[36mpandas._libs.index.DatetimeEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mindex.pyx:611\u001B[39m, in \u001B[36mpandas._libs.index.DatetimeEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mKeyError\u001B[39m: Timestamp('2023-11-30 23:06:00-0500', tz='US/Eastern')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\coin\\Lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:630\u001B[39m, in \u001B[36mDatetimeIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m    629\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Index.get_loc(\u001B[38;5;28mself\u001B[39m, key)\n\u001B[32m    631\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\coin\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3811\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[32m-> \u001B[39m\u001B[32m3812\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m   3814\u001B[39m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[32m   3815\u001B[39m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[32m   3816\u001B[39m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
      "\u001B[31mKeyError\u001B[39m: Timestamp('2023-11-30 23:06:00-0500', tz='US/Eastern')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[125]\u001B[39m\u001B[32m, line 136\u001B[39m\n\u001B[32m    133\u001B[39m     df_xetra = df_xetra.sort_index()\n\u001B[32m    135\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m df_trades, market \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m([df_nyse, df_xetra], [\u001B[33m'\u001B[39m\u001B[33mnyse\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mxetra\u001B[39m\u001B[33m'\u001B[39m]):\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m         df_event_study = filter_events(df_events, df_trades, get_pre = \u001B[32m0\u001B[39m).to_csv(\n\u001B[32m    137\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m../data/twitter/tiktok_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmarket\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_emotion_event_study.csv\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    138\u001B[39m             index=\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    139\u001B[39m         )\n\u001B[32m    140\u001B[39m         df_event_study_pre_post = filter_events(df_events, df_trades, get_pre=\u001B[32m1\u001B[39m).to_csv(\n\u001B[32m    141\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m../data/twitter/tiktok_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmarket\u001B[38;5;132;01m}\u001B[39;00m\u001B[33memotion_event_study_pre_post.csv\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    142\u001B[39m             index=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    144\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m[Info] Dataframe Inspection\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[125]\u001B[39m\u001B[32m, line 47\u001B[39m, in \u001B[36mfilter_events\u001B[39m\u001B[34m(df, df_trades, get_pre)\u001B[39m\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m date \u001B[38;5;129;01min\u001B[39;00m df.index:\n\u001B[32m     46\u001B[39m     day_data = df_trades[df_trades.index.date == date.date()]\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m     event_index = day_data.index.get_loc(date)\n\u001B[32m     49\u001B[39m     remaining = \u001B[38;5;28mlen\u001B[39m(day_data) - event_index\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m event_index >= est:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\coin\\Lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:632\u001B[39m, in \u001B[36mDatetimeIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m    630\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Index.get_loc(\u001B[38;5;28mself\u001B[39m, key)\n\u001B[32m    631\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m--> \u001B[39m\u001B[32m632\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(orig_key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n",
      "\u001B[31mKeyError\u001B[39m: Timestamp('2023-11-30 23:06:00-0500', tz='US/Eastern')"
     ]
    }
   ],
   "execution_count": 125
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
