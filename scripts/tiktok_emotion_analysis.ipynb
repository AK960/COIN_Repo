{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h1>TikTok Analysis - Part 2</h1>\n",
    "<h2><i>Event Study</i></h2>\n",
    "<h3>[2.1][Create Event Study Dataframe]</h3>"
   ],
   "id": "980d0e9b9c35aa78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:28:52.377291Z",
     "start_time": "2025-07-16T12:28:52.373613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Imports ###\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "### Set necessary workspace variables ###\n",
    "# Set execution type (to avoid repeating resource intensive operations)\n",
    "RUN_TYPE = 0 # set to 0 to avoid file creation process\n",
    "#RUN_TYPE = 1 # set to 1 to perform emotion analysis file creation process\n",
    "#RUN_TYPE = 2 # set to 2 to perform topic analysis file creation process\n",
    "\n",
    "# Define Ekman's emotions\n",
    "ekman_emotions = ['anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "\n",
    "# Timezones\n",
    "eastern = pytz.timezone(\"US/Eastern\")\n",
    "european = pytz.timezone(\"Europe/Berlin\")"
   ],
   "id": "2a9dc7038ed7f788",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-16T12:29:09.280098Z",
     "start_time": "2025-07-16T12:28:56.272595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Read necessary dataframes, set index, and convert timezones ###\n",
    "# Prep tweets\n",
    "df_tesla = pd.read_csv('../data/tiktok/tiktok_emotions.csv').dropna()\n",
    "df_tesla = df_tesla.drop(columns={'combined_text', 'combined_text_cleaned'}).rename(columns={\n",
    "    'total_engagement': 'engagement',\n",
    "    'combined_text_anger': 'anger',\n",
    "    'combined_text_fear': 'fear',\n",
    "    'combined_text_joy': 'joy',\n",
    "    'combined_text_sadness': 'sadness',\n",
    "    'combined_text_disgust': 'disgust',\n",
    "    'combined_text_surprise': 'surprise',\n",
    "    'combined_text_dominant_emotion': 'video_emotion'\n",
    "})\n",
    "df_tesla['timestamp'] = pd.to_datetime(df_tesla['timestamp'], format='%Y-%m-%d %H:%M:%S%z', utc=True)\n",
    "if df_tesla['timestamp'].dt.tz is None:\n",
    "    df_tesla['timestamp'] = df_tesla['timestamp'].dt.tz_localize(eastern)\n",
    "df_tesla['timestamp'] = df_tesla['timestamp'].dt.tz_convert(eastern)\n",
    "df_tesla = df_tesla.set_index('timestamp').sort_index()\n",
    "\n",
    "# Prep nyse data\n",
    "df_nyse = (pd.read_csv('../data/stocks/tesla_nyse_intraday_202305_202504-1m.csv')\n",
    "    .rename(columns={'Unnamed: 0': 'timestamp'})\n",
    "    .drop(columns={'open', 'high', 'low', 'close', 'volume'})\n",
    ")\n",
    "df_nyse['timestamp'] = pd.to_datetime(df_nyse['timestamp'], format='%Y-%m-%d %H:%M:%S%z', utc=True)\n",
    "if df_nyse['timestamp'].dt.tz is None:\n",
    "    df_nyse['timestamp'] = df_nyse['timestamp'].dt.tz_localize(eastern)\n",
    "df_nyse['timestamp'] = df_nyse['timestamp'].dt.tz_convert(eastern)\n",
    "df_nyse = df_nyse.set_index('timestamp').sort_index()\n",
    "\n",
    "# Prep xetra data\n",
    "df_xetra = (pd.read_csv('../data/stocks/tesla_xetra_intraday_202305_202504-1m.csv')\n",
    "    .rename(columns={'Unnamed: 0': 'timestamp'})\n",
    "    .drop(columns={'open', 'high', 'low', 'close', 'volume'})\n",
    ")\n",
    "df_xetra['timestamp'] = pd.to_datetime(df_xetra['timestamp'], format='%Y-%m-%d %H:%M:%S%z', utc=True)\n",
    "if df_xetra['timestamp'].dt.tz is None:\n",
    "    df_xetra['timestamp'] = df_xetra['timestamp'].dt.tz_localize(european)\n",
    "df_xetra['timestamp'] = df_xetra['timestamp'].dt.tz_convert(european)\n",
    "df_xetra = df_xetra.set_index('timestamp').sort_index()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:29:10.595771Z",
     "start_time": "2025-07-16T12:29:10.591160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Inspect dataframe ###\n",
    "print(\"[Tesla]\")\n",
    "print(\"Datentyp der Spalte:\", df_tesla.index.dtype)\n",
    "print(\"Tesla Columns:\", df_tesla.columns)\n",
    "print(\"Number Videos:\", len(df_tesla))\n",
    "\n",
    "### Inspect dataframe ###\n",
    "print(\"\\n[NYSE]\")\n",
    "print(\"Datentyp der Spalte:\", df_nyse.index.dtype)\n",
    "print(\"Tesla Columns:\" , df_nyse.columns)\n",
    "\n",
    "### Inspect dataframe ###\n",
    "print(\"\\n[XETRA]\")\n",
    "print(\"Datentyp der Spalte:\", df_xetra.index.dtype)\n",
    "print(\"Tesla Columns:\" , df_xetra.columns)"
   ],
   "id": "fbbf444b07280513",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tesla]\n",
      "Datentyp der Spalte: datetime64[ns, US/Eastern]\n",
      "Tesla Columns: Index(['engagement', 'anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise',\n",
      "       'video_emotion'],\n",
      "      dtype='object')\n",
      "Number Videos: 326\n",
      "\n",
      "[NYSE]\n",
      "Datentyp der Spalte: datetime64[ns, US/Eastern]\n",
      "Tesla Columns: Index(['minute_of_day', 'log_return', 'log_return_z', 'log_return_z_intraday',\n",
      "       'log_volume', 'log_volume_z', 'log_volume_z_intraday'],\n",
      "      dtype='object')\n",
      "\n",
      "[XETRA]\n",
      "Datentyp der Spalte: datetime64[ns, Europe/Berlin]\n",
      "Tesla Columns: Index(['minute_of_day', 'log_return', 'log_return_z', 'log_return_z_intraday',\n",
      "       'log_volume', 'log_volume_z', 'log_volume_z_intraday'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:29:14.665496Z",
     "start_time": "2025-07-16T12:29:14.543561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Group tweets to events with dominant emotion ###\n",
    "def compute_event_emotion(group):\n",
    "    total_engagement = group['engagement'].sum()\n",
    "    weighted_emo_scores = {\n",
    "        emotion: (group[emotion] * group['engagement']).sum() / total_engagement\n",
    "        for emotion in ekman_emotions\n",
    "    }\n",
    "    event_emotion = max(weighted_emo_scores, key=weighted_emo_scores.get)\n",
    "    return pd.Series({\n",
    "        **weighted_emo_scores,\n",
    "        'event_emotion': event_emotion\n",
    "    })\n",
    "\n",
    "if not ('event_emotion' in df_tesla.columns):\n",
    "    init_len = len(df_tesla)\n",
    "    # 1. Filter out videos outside market hours\n",
    "    df_tesla = df_tesla[\n",
    "        df_tesla.index.time >= pd.to_datetime(\"04:00\").time()\n",
    "    ]\n",
    "    df_tesla = df_tesla[\n",
    "        df_tesla.index.time <= pd.to_datetime(\"18:59\").time()\n",
    "    ]\n",
    "\n",
    "    # 2. Sort by time\n",
    "    df_tesla.sort_index()\n",
    "\n",
    "    # 3. Compute time difference between videos\n",
    "    df_tesla['time_diff'] = df_tesla.index.diff()\n",
    "\n",
    "    # 4. Cluster videos to events with time_diff >= 7h\n",
    "    df_tesla['new_event'] = df_tesla['time_diff'] > pd.Timedelta(hours=7)\n",
    "\n",
    "    # 5. Cumulate events in event_id\n",
    "    df_tesla['event_id'] = df_tesla['new_event'].cumsum()\n",
    "\n",
    "    # 6. Compute the dominant emotion for each event and merge back to df\n",
    "    df_event_emotions = (df_tesla\n",
    "        .drop(columns=['event_id'])\n",
    "        .groupby(df_tesla['event_id'], group_keys=False)\n",
    "        .apply(compute_event_emotion)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    df_tesla = (df_tesla\n",
    "        .reset_index()\n",
    "        .merge(\n",
    "            df_event_emotions[['event_id', 'event_emotion']],\n",
    "            on='event_id',\n",
    "            how='left'\n",
    "        )\n",
    "    )\n",
    "    df_tesla = df_tesla[['timestamp', 'event_id', 'event_emotion']]\n",
    "\n",
    "    # 7. Set event_time to time of the earliest video of the event (models from what point in time a reaction can occur)\n",
    "    df_tesla['event_time'] = df_tesla.groupby('event_id')['timestamp'].transform('min')\n",
    "\n",
    "    # 8. Filter duplicate events\n",
    "    df_tesla = df_tesla.drop_duplicates(subset=['event_id']).set_index('event_time').sort_index()\n",
    "    # 9. Show new length (plain events)\n",
    "    new_len = len(df_tesla)\n",
    "    rm_lines = init_len - new_len\n",
    "    print(\"[Tesla]\")\n",
    "    print(f\"[Info] Removed {rm_lines} tuples within events. New length (#videos): {new_len}\")\n",
    "    print(\"Datentyp der Spalte:\", df_tesla.index.dtype)\n",
    "    print(\"Tesla Columns:\", df_tesla.columns)"
   ],
   "id": "3014c83dd0b6a20c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tesla]\n",
      "[Info] Removed 188 tuples within events. New length (#videos): 138\n",
      "Datentyp der Spalte: datetime64[ns, US/Eastern]\n",
      "Tesla Columns: Index(['timestamp', 'event_id', 'event_emotion'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:38:10.997498Z",
     "start_time": "2025-07-16T12:38:10.987423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Create Event Study DataFrame (with actual, expected, abnormal return and volume ###\n",
    "# 1. Filter events based on overlaps of estimation and event windows\n",
    "# 2. Calculate expected and abnormal returns for each event\n",
    "def filter_events(df_vid: pd.DataFrame, df_fin: pd.DataFrame, fin_tz: str, vid_tz: str = 'US/Eastern', get_pre: int = 0):\n",
    "    # Define necessary vars\n",
    "    est = 300\n",
    "    obs = 180\n",
    "    gap = est + obs\n",
    "    results = []\n",
    "\n",
    "    # 1. Filter by overlaps\n",
    "    valid_events = []\n",
    "    last_event = None\n",
    "\n",
    "    # Add valid events to the list\n",
    "    for date in df_vid.index:\n",
    "        # First event always valid\n",
    "        if last_event is None:\n",
    "            valid_events.append(date)\n",
    "            last_event = date\n",
    "        else:\n",
    "            # If timeshift > gaps, keep the current event and set as new last_event\n",
    "            if (date - last_event) > pd.Timedelta(minutes=gap):\n",
    "                valid_events.append(date)\n",
    "                last_event = date\n",
    "\n",
    "    # Only keep valid events\n",
    "    df_vid = df_vid.loc[valid_events]\n",
    "    print(f\"[Overlap] New df_length: {len(df_vid)}\")\n",
    "\n",
    "    # 2. Align event_time with the next available trading timestamp\n",
    "    # 2.1 Convert both their respective timezone if not yet set\n",
    "    if df_vid.index.tz is None:\n",
    "        df_vid.index = df_vid.index.tz_localize(vid_tz)\n",
    "    elif df_vid.index.tz != vid_tz:\n",
    "        df_vid.index = df_vid.index.tz_convert(vid_tz)\n",
    "\n",
    "    # 2.2 Convert all timezones to utc (+00:00)\n",
    "    df_vid.index = df_vid.index.tz_convert('UTC')\n",
    "    df_fin.index = df_fin.index.tz_convert('UTC')\n",
    "\n",
    "    # 2.3 Align vid and fin times\n",
    "    for i, date in enumerate(df_vid.index):\n",
    "        if date not in df_fin.index:\n",
    "            future_times = df_fin.index[df_fin.index > date]\n",
    "            if not future_times.empty:\n",
    "                df_vid.index.values[i] = pd.Timestamp(future_times[0])\n",
    "\n",
    "    # 3. Filter events without sufficient estimation_window data\n",
    "    # 4. Filter events without sufficient event_window data\n",
    "    valid_events = []\n",
    "\n",
    "    for date in df_vid.index:\n",
    "        day_data = df_fin[df_fin.index.date == date.date()]\n",
    "        event_index = day_data.index.get_loc(date)\n",
    "\n",
    "        remaining = len(day_data) - event_index\n",
    "\n",
    "        if event_index >= est:\n",
    "            if remaining >= obs:\n",
    "                valid_events.append(date)\n",
    "\n",
    "    df_vid = df_vid.loc[valid_events]\n",
    "    print(f\"[Sum Data] New df_length = {len(df_vid)}\")\n",
    "\n",
    "    # 5. Event study variables\n",
    "    for event_row in df_vid.itertuples():\n",
    "        event_id = event_row.event_id\n",
    "        event_emo = event_row.event_emotion\n",
    "        event_time = event_row.Index\n",
    "\n",
    "        # 5.1 Compute estimation_ and event_window\n",
    "        estimation_window = df_fin.loc[\n",
    "            event_time - pd.Timedelta(minutes=est) : event_time - pd.Timedelta(minutes=1)\n",
    "        ]\n",
    "        event_window = df_fin.loc[\n",
    "            event_time - pd.Timedelta(minutes=get_pre * est): event_time + pd.Timedelta(minutes=obs - 1)\n",
    "        ]\n",
    "\n",
    "        # 5.2 Compute expected values based on estimation window\n",
    "        expected_return = estimation_window['log_return'].mean()\n",
    "        expected_volume = estimation_window['log_volume'].median() # robust against outliers, see Bamber (1987)\n",
    "        expected_volume_z_intraday = estimation_window['log_volume_z_intraday'].median()\n",
    "\n",
    "        for i, fin_row in enumerate(event_window.itertuples()):\n",
    "            if get_pre == 1:\n",
    "                i = int((fin_row.Index - event_time).total_seconds() // 60)\n",
    "\n",
    "            # 5.3 Compute actual and abnormal values\n",
    "            actual_return = fin_row.log_return\n",
    "            abnormal_return = actual_return - expected_return\n",
    "\n",
    "            actual_volume = fin_row.log_volume\n",
    "            abnormal_volume = actual_volume - expected_volume\n",
    "\n",
    "            actual_volume_z_intraday = fin_row.log_volume_z_intraday\n",
    "            abnormal_volume_z_intraday = actual_volume_z_intraday - expected_volume_z_intraday\n",
    "\n",
    "            results.append({\n",
    "                'event_id': event_id,\n",
    "                'event_fin_offset': i,\n",
    "                'event_time': event_time,\n",
    "                'fin_time': fin_row.Index,\n",
    "                'event_emo': event_emo,\n",
    "                'actual_return': actual_return,\n",
    "                'expected_return': expected_return,\n",
    "                'abnormal_return': abnormal_return,\n",
    "                'actual_volume': actual_volume,\n",
    "                'expected_volume': expected_volume,\n",
    "                'abnormal_volume': abnormal_volume,\n",
    "                'actual_volume_z_intraday': actual_volume_z_intraday,\n",
    "                'expected_volume_z_intraday': expected_volume_z_intraday,\n",
    "                'abnormal_volume_z_intraday': abnormal_volume_z_intraday\n",
    "            })\n",
    "\n",
    "    # 5.4 Make dataframe from results containing metrics\n",
    "    df_valid_events = pd.DataFrame(results)\n",
    "\n",
    "    return df_valid_events"
   ],
   "id": "55863f5dad6f14a8",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:43:12.847782Z",
     "start_time": "2025-07-16T12:41:59.270106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Create event_study and event_study_pre_post dataframes ###\n",
    "if RUN_TYPE == 0:\n",
    "    df_tesla = df_tesla[['event_id', 'event_emotion']] # index = event_time\n",
    "    df_nyse = df_nyse[['log_return', 'log_volume', 'log_volume_z_intraday']] # index = timestamp\n",
    "    df_xetra = df_xetra[['log_return', 'log_volume', 'log_volume_z_intraday']] # index = timestamp\n",
    "\n",
    "    for df_stock_exchange, name in zip([df_nyse, df_xetra], ['nyse', 'xetra']):\n",
    "        if name == 'df_nyse':\n",
    "            fin_timezone = pytz.timezone(\"US/Eastern\")\n",
    "        else:\n",
    "            fin_timezone = pytz.timezone(\"Europe/Berlin\")\n",
    "\n",
    "        # Create dfs (for inspection of results)\n",
    "        df_event_study = filter_events(df_tesla, df_stock_exchange, fin_tz=fin_timezone)\n",
    "        df_event_study_pre_post = filter_events(df_tesla, df_stock_exchange, fin_tz=fin_timezone, get_pre=1)\n",
    "\n",
    "        # Saving as file\n",
    "        df_event_study.to_csv(f'../data/tiktok/tiktok_{name}_event_study.csv')\n",
    "        df_event_study_pre_post.to_csv(f'../data/tiktok/tiktok_{name}_event_study_pre_post')\n",
    "\n",
    "        # Inspection of results\n",
    "        print(f\"[Info][{name}] Dataframe Inspection\")\n",
    "        print(f\"Emotions: {df_event_study['event_emo'].unique()}\")\n",
    "        emotion = df_event_study['event_emo'].unique()\n",
    "        for emo in emotion:\n",
    "            print(f\"  - {emo}: {len(df_event_study[df_event_study['event_emo'] == emo].groupby('event_id'))} events\")\n",
    "\n",
    "        print(f\"\\n[Info][{name}] Event Study DataFrame has {len(df_event_study)} entries.\")\n",
    "        print(f\"[Info] Event Study Pre/Post DataFrame has {len(df_event_study_pre_post)} entries.\\n\")\n",
    "        if (df_event_study_pre_post.columns == df_event_study.columns).all():\n",
    "            print(\"[Info] emotion_event_study(_pre_post).csv files have the following columns:\")\n",
    "            print(df_event_study.columns.tolist())"
   ],
   "id": "58572b9d2ba4ce6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Overlap] New df_length: 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aklei\\AppData\\Local\\Temp\\ipykernel_63452\\392983941.py:47: UserWarning: no explicit representation of timezones available for np.datetime64\n",
      "  df_vid.index.values[i] = pd.Timestamp(future_times[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sum Data] New df_length = 68\n",
      "[Overlap] New df_length: 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aklei\\AppData\\Local\\Temp\\ipykernel_63452\\392983941.py:47: UserWarning: no explicit representation of timezones available for np.datetime64\n",
      "  df_vid.index.values[i] = pd.Timestamp(future_times[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sum Data] New df_length = 68\n",
      "[Info][nyse] Dataframe Inspection\n",
      "Emotions: ['anger' 'fear' 'joy' 'sadness']\n",
      "  - anger: 8 events\n",
      "  - fear: 5 events\n",
      "  - joy: 53 events\n",
      "  - sadness: 2 events\n",
      "\n",
      "[Info][nyse] Event Study DataFrame has 12240 entries.\n",
      "[Info] Event Study Pre/Post DataFrame has 32640 entries.\n",
      "\n",
      "[Info] emotion_event_study(_pre_post).csv files have the following columns:\n",
      "['event_id', 'event_fin_offset', 'event_time', 'fin_time', 'event_emo', 'actual_return', 'expected_return', 'abnormal_return', 'actual_volume', 'expected_volume', 'abnormal_volume', 'actual_volume_z_intraday', 'expected_volume_z_intraday', 'abnormal_volume_z_intraday']\n",
      "[Overlap] New df_length: 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aklei\\AppData\\Local\\Temp\\ipykernel_63452\\392983941.py:47: UserWarning: no explicit representation of timezones available for np.datetime64\n",
      "  df_vid.index.values[i] = pd.Timestamp(future_times[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sum Data] New df_length = 66\n",
      "[Overlap] New df_length: 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aklei\\AppData\\Local\\Temp\\ipykernel_63452\\392983941.py:47: UserWarning: no explicit representation of timezones available for np.datetime64\n",
      "  df_vid.index.values[i] = pd.Timestamp(future_times[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sum Data] New df_length = 66\n",
      "[Info][xetra] Dataframe Inspection\n",
      "Emotions: ['anger' 'fear' 'joy' 'sadness']\n",
      "  - anger: 8 events\n",
      "  - fear: 5 events\n",
      "  - joy: 52 events\n",
      "  - sadness: 1 events\n",
      "\n",
      "[Info][xetra] Event Study DataFrame has 11880 entries.\n",
      "[Info] Event Study Pre/Post DataFrame has 31680 entries.\n",
      "\n",
      "[Info] emotion_event_study(_pre_post).csv files have the following columns:\n",
      "['event_id', 'event_fin_offset', 'event_time', 'fin_time', 'event_emo', 'actual_return', 'expected_return', 'abnormal_return', 'actual_volume', 'expected_volume', 'abnormal_volume', 'actual_volume_z_intraday', 'expected_volume_z_intraday', 'abnormal_volume_z_intraday']\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:28:17.372539Z",
     "start_time": "2025-07-16T11:28:17.368268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "berlin_time = pd.Timestamp('2023-11-15 09:00').tz_localize(european)\n",
    "ny_time = pd.Timestamp('2023-11-15 09:00').tz_localize(eastern)\n",
    "print(berlin_time)\n",
    "print(ny_time)\n",
    "print(berlin_time.tz_convert('UTC'))  # 2023-11-15 07:00:00+00:00\n",
    "print(ny_time.tz_convert('UTC'))     # 2023-11-15 13:00:00+00:00"
   ],
   "id": "61e862e87679f62d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-15 09:00:00+01:00\n",
      "2023-11-15 09:00:00-05:00\n",
      "2023-11-15 08:00:00+00:00\n",
      "2023-11-15 14:00:00+00:00\n"
     ]
    }
   ],
   "execution_count": 115
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
