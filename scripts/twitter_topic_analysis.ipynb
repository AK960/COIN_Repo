{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h1>Twitter Topic Emotion Analysis - Part 1</h1>\n",
    "<h2><i>Topic Modeling</i></h2>"
   ],
   "id": "5d76d4c232bdc0dc"
  },
  {
   "cell_type": "code",
   "id": "370db52a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T13:48:37.095276Z",
     "start_time": "2025-07-15T13:48:36.927221Z"
    }
   },
   "source": [
    "### Imports ###\n",
    "import pandas as pd\n",
    "from matplotlib import style\n",
    "from src.TextNormalizer import TextNormalizer\n",
    "from src.TimeNormalizer import TimeNormalizer\n",
    "style.use('ggplot')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "RUN_TYPE = 0 # Normal Mode (without file creation)\n",
    "# RUN_TYPE = 1 # Analysis Mode (with file creation)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aklei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T12:47:20.897687Z",
     "start_time": "2025-07-15T12:47:20.893589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Read Data for Topic Modeling ###\n",
    "if RUN_TYPE == 1:\n",
    "    df = pd.read_csv('../data/twitter/tweets_isTweet_emotions.csv')\n",
    "    df['combined_text'] = df['tweet_text'].fillna('') + '' + df['quoted_tweet_text'].fillna('')\n",
    "    df['combined_text'] = df[\"combined_text\"].apply(TextNormalizer.remove_noise)\n",
    "    df = (df[df['combined_text'] != ''])\n",
    "    df['combined_text'].head()"
   ],
   "id": "a3fc8af066617073",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T12:47:20.923017Z",
     "start_time": "2025-07-15T12:47:20.917532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Pre process text (embeddings) ###\n",
    "if RUN_TYPE == 1:\n",
    "    tweets = df['combined_text'].values.tolist()\n",
    "    print(f\"[Info] Embedding {len(tweets)} tweets ...\")\n",
    "\n",
    "    # 1. Embedding-Modell (vorher berechnen oder cachen)\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = embedder.encode(tweets, show_progress_bar=True)"
   ],
   "id": "2daca51fb872356",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T12:47:20.956980Z",
     "start_time": "2025-07-15T12:47:20.946087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Fit BERTopic model and print topic info ###\n",
    "if RUN_TYPE == 1:\n",
    "    # 2. UMAP (Reduktion für semantisch klarere Cluster)\n",
    "    umap_model = UMAP(n_neighbors=50, n_components=20, min_dist=0.05, metric=\"cosine\", random_state=42)\n",
    "\n",
    "    # 3. HDBSCAN (Cluster-Zahl steuern)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=20, cluster_selection_epsilon=0.3, metric=\"euclidean\", cluster_selection_method=\"eom\", prediction_data=True)\n",
    "\n",
    "    # 4. CountVectorizer\n",
    "    vectorizer_model = CountVectorizer(min_df=2, stop_words=\"english\")\n",
    "\n",
    "    # 5. Repräsentation (optional, für bessere Labels)\n",
    "    representation_model = KeyBERTInspired()\n",
    "\n",
    "    # 6.1 Tesla-related seed_words (use to find other topics and populate seed_topic_list)\n",
    "    seed_words = [\"tesla\", \"elon musk\", \"autopilot\", \"cybertruck\", \"model3\", \"gigafactory\", \"electric vehicle\", \"supercharger\", \"amp\"]\n",
    "\n",
    "    # 6.2 Tesla-related seed_topic_list (populated by finding broad topics with seed_words and wide clustering, now narrowing it down for accuracy)\n",
    "    # --> Should not choose to many words as seed_list can become blurry\n",
    "    seed_topic_list = [\n",
    "        [\"tesla\", \"elon musk\", \"autopilot\", \"cybertruck\", \"model3\", \"gigafactory\", \"electric vehicle\", \"supercharger\"],\n",
    "        [\"president\", \"trump\", \"government\", \"election\", \"republican\", \"democrat\", \"vote\", \"ballot\"],\n",
    "        [\"judge\", \"activist\", \"illegal\"],\n",
    "        [\"doge\", \"dogefather\"],\n",
    "        [\"spacex\", \"launch\", \"falcon\", \"orbit\", \"mars\"],\n",
    "        [\"bitcoin\", \"dogecoin\"],\n",
    "        [\"starlink\", \"broadband\", \"highspeed\"],\n",
    "        [\"fertility\", \"birthrate\", \"population\", \"births\", \"demographic\"],\n",
    "        [\"twitter\", \"tweet\", \"ban\", \"free speech\", \"grok\", \"grokai\"],\n",
    "        [\"crypto\", \"bitcoin\", \"dogecoin\"],\n",
    "        [\"white\", \"farmers\", \"south africa\", \"field\", \"genocide\"],\n",
    "        [\"afd\", \"german\", \"coalition\", \"berlin\"]\n",
    "    ]\n",
    "\n",
    "    # 7. Topic-Modell\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedder,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "        calculate_probabilities=True,\n",
    "        #seed_topic_list=seed_words, # used to populate seed_topic_list\n",
    "        seed_topic_list=seed_topic_list,\n",
    "        nr_topics=\"auto\", # Automatically generate topic count\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # 8. Fitting\n",
    "    topics, probs = topic_model.fit_transform(tweets, embeddings)\n",
    "\n",
    "    # 9. Reduce Outliers\n",
    "    #new_topics = topic_model.reduce_outliers(tweets, topics, strategy=\"embeddings\") # Method to reduce outliers\n",
    "    new_topics = topic_model.reduce_outliers(tweets, topics, probabilities=probs, strategy=\"probabilities\") # Method to reduce outliers\n",
    "    topic_model.update_topics(tweets, topics=new_topics)\n",
    "\n",
    "    # 9. Show Topic Info\n",
    "    topic_model.get_topic_info()"
   ],
   "id": "dc95dac4c6c198db",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T12:47:21.606596Z",
     "start_time": "2025-07-15T12:47:21.601301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Create Dataframe from Topic List and Filter Tesla Topic ###\n",
    "if RUN_TYPE == 1:\n",
    "    # 1. Get topic info\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "\n",
    "    # 2. Create dataframe\n",
    "    df_topics = pd.DataFrame(topic_info)[['Topic', 'Representation']]\n",
    "\n",
    "    # 3. Get topic ids with tesla related topics\n",
    "    tesla_key = \"tesla\"\n",
    "    tesla_topic_ids = []\n",
    "\n",
    "    for _, row in df_topics.iterrows():\n",
    "        if tesla_key in row['Representation']:\n",
    "            tesla_topic_ids.append(row['Topic'])\n",
    "\n",
    "    print(f\"Tesla-Topic-IDs: {tesla_topic_ids}\")\n",
    "\n",
    "    # 4. Filter df by documents\n",
    "    doc_info = topic_model.get_document_info(tweets)\n",
    "    df['topics'] = doc_info['Topic']\n",
    "    df_tesla = df[df['topics'].isin(tesla_topic_ids)]\n",
    "\n",
    "    # 5. Drop unnecessary columns and safe to csv\n",
    "    df_tesla = df_tesla[['tweet_id', 'createdAt', 'topics', 'combined_text', 'tweet_text_dominant_emotion', 'quoted_tweet_id', 'quoted_tweet_text_dominant_emotion']]\n",
    "    df_tesla.to_csv('../data/twitter/tweets_isTweet_emotions_tesla.csv', index=False)\n",
    "    print(f\"Found Tesla-Tweets: {len(df_tesla)}\")"
   ],
   "id": "f8355a011ea3d25",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h1>Twitter Topic Emotion Analysis - Part 1</h1>\n",
    "<h2><i>Event Study</i></h2>\n",
    "<p>\n",
    "    In this section, the two event studies of the emotion and topic data will be combined in order to examine, if we can observe an effect of the topic data with certain emotions on the stock price / trading volume on the NYSE and Xetra.\n",
    "</p>"
   ],
   "id": "3bb0e20e8cd9a016"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T13:48:42.851702Z",
     "start_time": "2025-07-15T13:48:42.177172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Read necessary dataframes, set index, and convert timezones ###\n",
    "# Prep tweets\n",
    "df_tesla = (pd.read_csv('../data/twitter/tweets_isTweet_emotions_tesla.csv')\n",
    "    .dropna()\n",
    "    .rename(columns={'createdAt': 'timestamp'})\n",
    ")\n",
    "df_tesla = TimeNormalizer.normalize_time(df_tesla, 'timestamp', 'US/Eastern').set_index('timestamp')\n",
    "\n",
    "# Prep nyse data\n",
    "df_nyse = pd.read_csv('../data/stocks/tsla_intraday_202305_202504-1m.csv')\n",
    "\n",
    "# Prep xetra data\n",
    "df_xetra = pd.read_csv('../data/stocks/TSL0_intraday_230501_250501-1m.csv')\n",
    "\n",
    "df_tesla.head()\n"
   ],
   "id": "cf8df6cbd49bfa1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                      tweet_id  topics  \\\n",
       "timestamp                                                \n",
       "2025-04-24 18:30:51-04:00  1915533926749364264     5.0   \n",
       "2025-04-24 12:39:13-04:00  1915445436116267382     5.0   \n",
       "2025-04-24 12:37:58-04:00  1915445119509209426     5.0   \n",
       "2025-04-24 10:48:09-04:00  1915417484380803137    30.0   \n",
       "2025-04-22 15:47:24-04:00  1914768017738604728     5.0   \n",
       "\n",
       "                                                               combined_text  \\\n",
       "timestamp                                                                      \n",
       "2025-04-24 18:30:51-04:00  worth itfsd supervised 99month effectively 333...   \n",
       "2025-04-24 12:39:13-04:00  act blue guilty widespread criminal identity t...   \n",
       "2025-04-24 12:37:58-04:00  wowso let get straight socalled maryland dad k...   \n",
       "2025-04-24 10:48:09-04:00  scam centurya new legal letter aimed openai la...   \n",
       "2025-04-22 15:47:24-04:00  starlink available 120 countriesstarlinks high...   \n",
       "\n",
       "                          tweet_text_dominant_emotion  quoted_tweet_id  \\\n",
       "timestamp                                                                \n",
       "2025-04-24 18:30:51-04:00                         joy     1.915527e+18   \n",
       "2025-04-24 12:39:13-04:00                       anger     1.915418e+18   \n",
       "2025-04-24 12:37:58-04:00                         joy     1.915423e+18   \n",
       "2025-04-24 10:48:09-04:00                       anger     1.915004e+18   \n",
       "2025-04-22 15:47:24-04:00                         joy     1.914728e+18   \n",
       "\n",
       "                          quoted_tweet_text_dominant_emotion  \n",
       "timestamp                                                     \n",
       "2025-04-24 18:30:51-04:00                                joy  \n",
       "2025-04-24 12:39:13-04:00                                joy  \n",
       "2025-04-24 12:37:58-04:00                                joy  \n",
       "2025-04-24 10:48:09-04:00                                joy  \n",
       "2025-04-22 15:47:24-04:00                                joy  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>topics</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>tweet_text_dominant_emotion</th>\n",
       "      <th>quoted_tweet_id</th>\n",
       "      <th>quoted_tweet_text_dominant_emotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-04-24 18:30:51-04:00</th>\n",
       "      <td>1915533926749364264</td>\n",
       "      <td>5.0</td>\n",
       "      <td>worth itfsd supervised 99month effectively 333...</td>\n",
       "      <td>joy</td>\n",
       "      <td>1.915527e+18</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-24 12:39:13-04:00</th>\n",
       "      <td>1915445436116267382</td>\n",
       "      <td>5.0</td>\n",
       "      <td>act blue guilty widespread criminal identity t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>1.915418e+18</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-24 12:37:58-04:00</th>\n",
       "      <td>1915445119509209426</td>\n",
       "      <td>5.0</td>\n",
       "      <td>wowso let get straight socalled maryland dad k...</td>\n",
       "      <td>joy</td>\n",
       "      <td>1.915423e+18</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-24 10:48:09-04:00</th>\n",
       "      <td>1915417484380803137</td>\n",
       "      <td>30.0</td>\n",
       "      <td>scam centurya new legal letter aimed openai la...</td>\n",
       "      <td>anger</td>\n",
       "      <td>1.915004e+18</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-22 15:47:24-04:00</th>\n",
       "      <td>1914768017738604728</td>\n",
       "      <td>5.0</td>\n",
       "      <td>starlink available 120 countriesstarlinks high...</td>\n",
       "      <td>joy</td>\n",
       "      <td>1.914728e+18</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
