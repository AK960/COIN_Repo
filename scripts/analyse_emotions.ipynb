{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Emotion Analysis",
   "id": "e2ac6fbebcfa0983"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T17:50:27.127114Z",
     "start_time": "2025-06-01T17:50:27.110660Z"
    }
   },
   "source": [
    "### Imports\n",
    "\n",
    "import pandas as pd\n",
    "from lxml.parser import result\n",
    "from pandas import json_normalize, Series\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from sympy.strategies.core import switch\n",
    "style.use('ggplot')\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import json\n",
    "from json import loads, dumps\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T18:20:57.377476Z",
     "start_time": "2025-06-01T18:20:52.723086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Prepare dataframe for analysis\n",
    "# Target structure:\n",
    "# [tweet_date,\n",
    "# tweet_id, tweet_text, tweet_text_cleaned, tweet_emotion1-6, tweet_dominant_emotion,\n",
    "# quoted_tweet_id, quoted_tweet_text, quoted_tweet_text_cleaned, quoted_emotion1-6, quoted_dominant_emotion]\n",
    "# Import dataset\n",
    "df_tweets = pd.read_csv(\n",
    "    '../data/twitter/tweets_isTweet.csv',\n",
    "    dtype={'id': 'object'},\n",
    "    low_memory=False\n",
    ")\n",
    "df_tweets = df_tweets[['id', 'createdAt', 'text', 'quoted_tweet']]\n",
    "df_tweets = df_tweets.rename(columns={'id': 'tweet_id', 'text': 'tweet_text'})\n",
    "\n",
    "# Normalize json column\n",
    "quoted_tweets_normalized = pd.json_normalize(\n",
    "    df_tweets['quoted_tweet'].apply(\n",
    "        lambda x: json.loads(x) if pd.notna(x) and isinstance(x, str) else None\n",
    "    )\n",
    ")\n",
    "quoted_tweets_normalized = quoted_tweets_normalized.rename(columns={'id': 'quoted_tweet_id', 'text': 'quoted_tweet_text'})\n",
    "\n",
    "# Link by index\n",
    "df_tweets.index = quoted_tweets_normalized.index\n",
    "\n",
    "# Concat both dataframes\n",
    "df_tweets_normalized = pd.concat([\n",
    "    df_tweets[['tweet_id', 'createdAt', 'tweet_text']],\n",
    "    quoted_tweets_normalized[['quoted_tweet_id', 'quoted_tweet_text']]\n",
    "], axis=1)\n",
    "\n",
    "# View data\n",
    "df_tweets_normalized.head()"
   ],
   "id": "c37722425b3f995",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              tweet_id                       createdAt  \\\n",
       "0  1917726279195058338  Wed Apr 30 23:42:29 +0000 2025   \n",
       "1  1917693698281787564  Wed Apr 30 21:33:01 +0000 2025   \n",
       "2  1917225430702240067  Tue Apr 29 14:32:17 +0000 2025   \n",
       "3  1917114631287718009  Tue Apr 29 07:12:01 +0000 2025   \n",
       "4  1917103264417649121  Tue Apr 29 06:26:50 +0000 2025   \n",
       "\n",
       "                tweet_text      quoted_tweet_id  \\\n",
       "0  https://t.co/U6tI9pdin6                  NaN   \n",
       "1  https://t.co/1c1WjFpOva                  NaN   \n",
       "2       This is a big deal  1917223651625099407   \n",
       "3  https://t.co/6xSd8l67FN                  NaN   \n",
       "4                     Whoa  1917011279757066291   \n",
       "\n",
       "                                   quoted_tweet_text  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  Last week, Treasury went live with its first a...  \n",
       "3                                                NaN  \n",
       "4  ðŸš¨THE INVISIBLE PUPPET MASTERS: AI'S DISTURBING...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>quoted_tweet_id</th>\n",
       "      <th>quoted_tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1917726279195058338</td>\n",
       "      <td>Wed Apr 30 23:42:29 +0000 2025</td>\n",
       "      <td>https://t.co/U6tI9pdin6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1917693698281787564</td>\n",
       "      <td>Wed Apr 30 21:33:01 +0000 2025</td>\n",
       "      <td>https://t.co/1c1WjFpOva</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1917225430702240067</td>\n",
       "      <td>Tue Apr 29 14:32:17 +0000 2025</td>\n",
       "      <td>This is a big deal</td>\n",
       "      <td>1917223651625099407</td>\n",
       "      <td>Last week, Treasury went live with its first a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1917114631287718009</td>\n",
       "      <td>Tue Apr 29 07:12:01 +0000 2025</td>\n",
       "      <td>https://t.co/6xSd8l67FN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1917103264417649121</td>\n",
       "      <td>Tue Apr 29 06:26:50 +0000 2025</td>\n",
       "      <td>Whoa</td>\n",
       "      <td>1917011279757066291</td>\n",
       "      <td>ðŸš¨THE INVISIBLE PUPPET MASTERS: AI'S DISTURBING...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### Pre-process data for the analysis\n",
    "## Variables\n",
    "ekman_emotions = ['anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "\n",
    "## Classifier\n",
    "# Load Hugging Face's emotion classifier\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/bert-base-uncased-emotion\", top_k=None, device=0 if device == \"cuda\" else -1)\n",
    "## Functions\n",
    "# Removing noise from the text\n",
    "def remove_noise(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https\\S+|www\\S+httpss\\S+\", '', text, flags=re.MULTILINE) # Remove Url\n",
    "    text = re.sub(r\"\\@w+|\\#\", '', text) # remove @ and #\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text) # remove punctuation\n",
    "    text_tokens = text.split()\n",
    "    filtered_text = [w for w in text_tokens if not w in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Reduction of dimensionality by abstracting word to word stem\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    stemmed_text = [stemmer.stem(word) for word in text]\n",
    "    return stemmed_text\n",
    "\n",
    "def compute_emotions(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(f\"Invalid text: {text[:10]}...\")\n",
    "        return {emotion: 0.0 for emotion in ekman_emotions}\n",
    "\n",
    "    try:\n",
    "        cleaned_text = remove_noise(text) # remove noise from text\n",
    "        cleaned_text = stem_words(cleaned_text) # reduce dimensionality\n",
    "        results = classifier(cleaned_text)[0]\n",
    "        emotion_scores = {result['label']: result['score'] for emotion in ekman_emotions}\n",
    "\n",
    "        return {emotion: emotion_scores.get(emotion,  for\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing text: {text[:10]}...\\nError: {e}\")\n",
    "        return {emotion: 0.0 for emotion in ekman_emotions}\n"
   ],
   "id": "c3f0b8cab02dc451"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
