{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ac6fbebcfa0983",
   "metadata": {},
   "source": [
    "<h1>Emotion Analysis - Part 1</h1>\n",
    "<h2><i>Data Preparation and Emotion Analysis with Text-Classification Model</i></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1133e22ef4f261",
   "metadata": {},
   "source": [
    "<h3>[1.1][Imports]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Imports ###\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('ggplot')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "from scipy import stats\n",
    "from scipy.stats import f_oneway\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "### Set necessary workspace variables ###\n",
    "\n",
    "# Set execution type (to avoid repeating resource intensive operations)\n",
    "#RUN_TYPE = 0 # set to 0 to avoid file creation process\n",
    "RUN_TYPE = 1 # set to 1 to perform emotion analysis file creation process\n",
    "#RUN_TYPE = 2 # set to 2 to perform topic analysis file creation process\n",
    "\n",
    "# Define Ekman's emotions \n",
    "ekman_emotions = ['anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise']\n",
    "\n",
    "# Timezones\n",
    "eastern = pytz.timezone(\"US/Eastern\")\n",
    "european = pytz.timezone(\"Europe/Berlin\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac53443b9391eb8b",
   "metadata": {},
   "source": [
    "<h3>[1.2][Data Preparation & Cleaning]</h3>\n",
    "<p>Normalizing the quoted_tweets-dictionary to create a dataframe that contains the text of the quoted and origional tweet.</p>\n",
    "<p>The following cell contains functions for the second step of data preparation, namely removing noise, truncating text, stemming words, and subsequently performing the emotion analysis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "id": "c37722425b3f995",
   "metadata": {},
   "source": [
    "### Process File with Tweet-Data (load and extract necessary columns) ### \n",
    "## Import dataset\n",
    "df_tweets = pd.read_csv(\n",
    "    '../data/twitter/tweets_isTweet.csv',\n",
    "    dtype={'id': 'object'},\n",
    "    low_memory=False\n",
    ")\n",
    "## Parameter for weighing emotions per event later in Event Study\n",
    "df_tweets['total_engagement'] = (df_tweets['retweetCount'] +\n",
    "                                 df_tweets['likeCount'] +\n",
    "                                 df_tweets['replyCount'] +\n",
    "                                 df_tweets['quoteCount'] +\n",
    "                                 df_tweets['viewCount'])\n",
    "df_tweets = df_tweets[['id', 'createdAt', 'text', 'quoted_tweet', 'total_engagement']]\n",
    "df_tweets = df_tweets.rename(columns={'id': 'tweet_id', 'text': 'tweet_text'})\n",
    "\n",
    "## Normalize json column\n",
    "quoted_tweets_normalized = pd.json_normalize(\n",
    "    df_tweets['quoted_tweet'].apply(\n",
    "        lambda x: json.loads(x) if pd.notna(x) and isinstance(x, str) else None\n",
    "    )\n",
    ")\n",
    "quoted_tweets_normalized = quoted_tweets_normalized.rename(columns={\n",
    "    'id': 'quoted_tweet_id',\n",
    "    'text': 'quoted_tweet_text'\n",
    "})\n",
    "\n",
    "## Link by index\n",
    "df_tweets.index = quoted_tweets_normalized.index\n",
    "\n",
    "## Concat both dataframes\n",
    "df_tweets_normalized = pd.concat([\n",
    "    df_tweets[['tweet_id', 'createdAt', 'tweet_text', 'total_engagement']],\n",
    "    quoted_tweets_normalized[['quoted_tweet_id', 'quoted_tweet_text']]\n",
    "], axis=1)\n",
    "\n",
    "## View data\n",
    "print(df_tweets.columns)\n",
    "#df_tweets_normalized.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3f0b8cab02dc451",
   "metadata": {},
   "source": [
    "### Prepare data for analysis (append emotions and topics) ###\n",
    "\n",
    "# 1. Set Classifier\n",
    "# Load Hugging Face's emotion classifier\n",
    "print(\"[Info]\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/bert-base-uncased-emotion\", top_k=None, device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "# 2. Functions to clean and process text data \n",
    "# Removing noise from the text\n",
    "def remove_noise(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https\\S+|www\\S+httpss\\S+\", '', text, flags=re.MULTILINE) # Remove Url\n",
    "    text = re.sub(r\"\\@w+|\\#\", '', text) # remove @ and #\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text) # remove punctuation\n",
    "    text_tokens = text.split()\n",
    "    filtered_text = [w for w in text_tokens if not w in stop_words] # remove stopwords\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Reduction of dimensionality by abstracting word to word stem and truncating text\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    words = text.split()\n",
    "    stemmed_text = [stemmer.stem(word) for word in words] # abstract to word stem\n",
    "    return stemmed_text\n",
    "\n",
    "def truncate_text(text, max_length=512):\n",
    "    words = text.split()\n",
    "    return \" \".join(words[:max_length])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b52bb378",
   "metadata": {},
   "source": [
    "<h4>[1.2.1] Append Emotions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "id": "84f9ef5c",
   "metadata": {},
   "source": [
    "# 3. Computing and appending emotions to dataframe\n",
    "def compute_emotions(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        print(\"[ComputeEmotions] Empty cell after data cleaning. Returning 0.0 for all emotions.\")\n",
    "        return {emotion: 0.0 for emotion in ekman_emotions}\n",
    "\n",
    "    try:\n",
    "        # Classify emotions using the Hugging Face pipeline and handle errors\n",
    "        results = classifier(text)[0]\n",
    "        if not results or not isinstance(results, list) or len(results[0]) == 0:\n",
    "            return {emotion: 0.0 for emotion in ekman_emotions}\n",
    "\n",
    "        emotion_scores = {result['label']: result['score'] for result in results}\n",
    "        return {emotion: emotion_scores.get(emotion, 0.0) for emotion in ekman_emotions}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ComputeEmotions] Error while processing text: {text[:20]}... Error: {e}\")\n",
    "        return {emotion: 0.0 for emotion in ekman_emotions}\n",
    "\n",
    "def append_emotions(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"[AppendEmotions] Column '{text_column}' not found in DataFrame.\")\n",
    "    \n",
    "    print(\"[AppendEmotions] Computing emotions for column:\", text_column)\n",
    "\n",
    "    cleaned_column = f\"{text_column}_cleaned\"\n",
    "    df[cleaned_column] = df[text_column].apply(\n",
    "        lambda x: \" \".join(stem_words(remove_noise(x))) if isinstance(x, str) and x.strip() else \"\"\n",
    "    )\n",
    "\n",
    "    # Truncate text if cleaned text exceeds 512 tokens\n",
    "    if (df[cleaned_column].str.split().str.len() > 512).any():\n",
    "        print(\"[AppendEmotions] At least one row with more than 512 tokens - truncating text ...\")\n",
    "        df[cleaned_column] = df[cleaned_column].apply(lambda x: truncate_text(x, max_length=512))\n",
    "\n",
    "    emotion_scores = [compute_emotions(text) for text in tqdm(df[cleaned_column], desc=\"[AppendEmotions] Processing emotions\")]\n",
    "    emotions_df = pd.DataFrame(emotion_scores)\n",
    "    emotions_df.index = df.index\n",
    "    emotions_df.columns = [f\"{text_column}_{emotion}\" for emotion in ekman_emotions]\n",
    "    \n",
    "    # Add dominant emotion column\n",
    "    dominant = emotions_df.idxmax(axis=1).apply(lambda x: x.split('_')[-1])\n",
    "    all_zero = emotions_df.eq(0.0).all(axis=1)\n",
    "    dominant[all_zero] = np.nan\n",
    "    emotions_df[f\"{text_column}_dominant_emotion\"] = dominant\n",
    "\n",
    "    # Insert right hand of input text_column\n",
    "    insert_at = df.columns.get_loc(text_column) + 1\n",
    "\n",
    "    # DataFrame in drei Teile splitten und zusammenf√ºgen\n",
    "    left = df.iloc[:, :insert_at]\n",
    "    right = df.iloc[:, insert_at:].drop(columns=[cleaned_column], errors='ignore')\n",
    "    result_df = pd.concat([left, df[[cleaned_column]], emotions_df, right], axis=1)\n",
    "\n",
    "    return result_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74496065f93a1e12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T20:40:48.007013Z",
     "start_time": "2025-06-23T20:40:48.003857Z"
    }
   },
   "source": [
    "<p>This part is only run once to create the new csv-file. Subsequently, the further analysis is performed on the new dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "id": "21425336e446c3b2",
   "metadata": {},
   "source": [
    "### Create Datasets: Performing Emotion and Content Analysis ###\n",
    "\n",
    "# Perform emotion analysis for columns specified and safe as new csv-file\n",
    "if RUN_TYPE == 1:\n",
    "    for col in ['tweet_text']:#, 'quoted_tweet_text']:\n",
    "        df_tweets_normalized = append_emotions(df_tweets_normalized, text_column=col)\n",
    "        \n",
    "    df_tweets_normalized.to_csv('../data/twitter/tweets_isTweet_emotions.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b358fcc34cf7a087",
   "metadata": {},
   "source": [
    "<h1>Emotion Analysis - Part 2</h1>\n",
    "<h2><i>Formatting & Data Exploration</i></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b597a35f3617558",
   "metadata": {},
   "source": [
    "<h3>[2.1][Data Formatting]</h3>"
   ]
  },
  {
   "cell_type": "code",
   "id": "4852931cdb52304a",
   "metadata": {},
   "source": [
    "### Read necessary datasets (tweets & financial) and convert to the same timezone ###\n",
    "\n",
    "# Set timezone \n",
    "eastern = pytz.timezone(\"US/Eastern\")\n",
    "\n",
    "# Tweet data\n",
    "df_tweets_normalized = pd.read_csv('../data/twitter/tweets_isTweet_emotions.csv')[\n",
    "    ['tweet_id',\n",
    "     'createdAt',\n",
    "     'total_engagement',\n",
    "     'tweet_text_anger',\n",
    "     'tweet_text_fear',\n",
    "     'tweet_text_joy',\n",
    "     'tweet_text_sadness',\n",
    "     'tweet_text_disgust',\n",
    "     'tweet_text_surprise',\n",
    "     'tweet_text_dominant_emotion'\n",
    "    ]\n",
    "].dropna()\n",
    "\n",
    "df_tweets_normalized = df_tweets_normalized.rename(\n",
    "    columns={\n",
    "        'tweet_id': 'id',\n",
    "        'total_engagement': 'engagement',\n",
    "        'createdAt': 'timestamp',\n",
    "        'tweet_text_anger': 'anger',\n",
    "        'tweet_text_fear': 'fear',\n",
    "        'tweet_text_joy': 'joy',\n",
    "        'tweet_text_sadness': 'sadness',\n",
    "        'tweet_text_disgust': 'disgust',\n",
    "        'tweet_text_surprise': 'surprise',\n",
    "        'tweet_text_dominant_emotion': 'tweet_emotion'\n",
    "    }\n",
    ")\n",
    "\n",
    "df_tweets_normalized['timestamp'] = pd.to_datetime(\n",
    "    df_tweets_normalized['timestamp'],\n",
    "    format=\"%a %b %d %H:%M:%S %z %Y\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "df_tweets_normalized['timestamp'] = df_tweets_normalized['timestamp'].dt.tz_convert(\n",
    "    tz=eastern\n",
    ")\n",
    "\n",
    "# Stock data\n",
    "df_stock_data = pd.read_csv('../legacy/data/tsla_intraday_202305_202504-1m.csv')\n",
    "\n",
    "df_stock_data = df_stock_data.rename(\n",
    "    columns={'Unnamed: 0': 'timestamp'}\n",
    ")\n",
    "\n",
    "df_stock_data['timestamp'] = pd.to_datetime(\n",
    "    df_stock_data['timestamp']\n",
    ").dt.tz_localize(\n",
    "    tz=eastern\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6c0c4f1",
   "metadata": {},
   "source": [
    "<h3>[2.2][Log Transformation]</h3>\n",
    "<p>\n",
    "    To analyse the fluctuations in return and volume, we use a logarithmized scale. This has some advantages:\n",
    "    <!--\n",
    "        Note for me:\n",
    "        The return is the difference between prev and actual closing price:\n",
    "        ln(actual/prev) = ln(actual) - ln(prev) // continuous returns are computed based on logarithmized price change\n",
    "        For volume, we are interested in absolute values that where traded and not in relative changes, thus, we do not compare a actual and prev value. The volume can be 0 for some points in time, thus we must use log1p to cope with the case where log(0) would happen. Logarithmizing scales of the dispersed data to make it comparable.\n",
    "        log1p(x) = log(1 + x) \n",
    "        => for small values of x, operates like a linear transformation\n",
    "        => for large values of x, operates like a log transformation \n",
    "    -->\n",
    "    <ul>\n",
    "        <li>\n",
    "            Stationary time-series: When observing the absolute trend, the price fluctuates and rises / falls over time (the mean changes over time). However, when looking at the changes per minute, the values fluctuate around a constant means.\n",
    "        </li>\n",
    "        <li>\n",
    "            Additivity: By adding the log_returns (stationary values per minute), we can compute the overall effect within a time window. The single fluctuations around the constant mean add up to show a general fluctuation from a constant mean during the observed time window.\n",
    "        </li>\n",
    "        <li>\n",
    "            Symmetry of up- and downwards trends: The return (relative change) is not symmetrical, the log_return is. In example:<br>\n",
    "            return: 10 -> 11 = 1.1 ; 11 -> 10 = 0.91 <br>\n",
    "            log_return: 10 -> 11 = 0.095 ; 11 -> 10 = -0.095\n",
    "        </li>\n",
    "        <li>Closer to being normally distributed (compared to absolute returns)\n",
    "            <ul>\n",
    "                <li>Central limit theorem: when adding many independent variables, their sum approximates a normal distribution</li>\n",
    "                <li>Here: as trends are the sum of many independet factors, the sum of their deviation will approximate a normal distribution</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "id": "f3d5e99f",
   "metadata": {},
   "source": [
    "### Computing log return/volume and intraday profile ###\n",
    "\n",
    "# Compute minute_of_day and intraday_profile for seasonality analysis\n",
    "df_stock_data['minute_of_day'] = df_stock_data['timestamp'].dt.hour * 60 + df_stock_data['timestamp'].dt.minute\n",
    "\n",
    "# Return\n",
    "df_stock_data['log_return'] = np.log(\n",
    "    df_stock_data['close'] / df_stock_data['close'].shift(1)\n",
    ")\n",
    "\n",
    "df_stock_data['log_return_z'] = (df_stock_data['log_return'] - df_stock_data['log_return'].mean()) / df_stock_data['log_return'].std()\n",
    "\n",
    "\n",
    "# Volume (use natural logarithm with 1-shift to avoid log(0) issues - see Bamberg (1987)))\n",
    "df_stock_data['log_volume'] = np.log1p( # Using log1p to avoid issues with log(0), i.e., when no trades were made\n",
    "    df_stock_data['volume']             # => log1p(x) = log(x + 1)\n",
    ")\n",
    "\n",
    "intraday_profile = df_stock_data.groupby('minute_of_day')[['log_return', 'log_volume']].agg(['mean', 'std']).reset_index()\n",
    "df_stock_data['log_volume_z_intraday'] = df_stock_data.groupby('minute_of_day')['log_volume'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "df_stock_data['log_return_z_intraday'] = df_stock_data.groupby('minute_of_day')['log_return'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "\n",
    "df_stock_data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea5999e2",
   "metadata": {},
   "source": [
    "# Test: Get median of log volume \n",
    "\n",
    "vol_list = df_stock_data['log_volume'].tolist()\n",
    "median_log_volume = np.median(vol_list)\n",
    "print(f\"[Median Log Volume] Median of log volume: {median_log_volume:.3f}\")\n",
    "\n",
    "mean_log_volume = np.mean(vol_list)\n",
    "print(f\"[Mean Log Volume] Mean of log volume: {mean_log_volume:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5f1d3514",
   "metadata": {},
   "source": [
    "<h3>[2.3][Testing for Seasonality]</h3>\n",
    "<p>\n",
    "    Before analyzing whether or not we can observe a real effect of emotions on the stock, we must cancel out a possible bias, namely the intraday seasonality. The term describes the effect that the stock price / trading volume is influenced by the time of day, where the two metrics experience higher volatility at market opening and closing hours than during the day.\n",
    "</p>\n",
    "<p>\n",
    "    To test for this effect, we must group the return / volume by the time of day and compare the means / variances of the groups. With the F-test we can check whether or not the mean values differ significantly between the different groups (here: hours of the day).\n",
    "</p>\n",
    "<p>\n",
    "    To apply the F-test, we can use the f_oneway-Method, which tests the null hypothesis that two or more groups have the same population mean. The test is applied to samples of two or more groups, possibly with different sizes. Here, it is applied to the data grouped by hours of the day and test whether the different goups (hours) have the same population mean. If a group mean significantly differs from the other groups means, we can conclude intraday effects.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "id": "c05f99c0",
   "metadata": {},
   "source": [
    "### Plotting intraday profile of means ### \n",
    "\n",
    "print(\"[Intraday Profile] Visualization of intraday seasonality: Course of means per minute of day\")\n",
    "\n",
    "# Convert minute of the day to time_label \n",
    "intraday_profile['hour'] = intraday_profile['minute_of_day'] // 60\n",
    "intraday_profile['minute'] = intraday_profile['minute_of_day'] % 60\n",
    "intraday_profile['time_label'] = intraday_profile['hour'].astype(str) + ':' + intraday_profile['minute'].astype(str).str.zfill(2)\n",
    "\n",
    "# Subplot with two y-axes for different scales\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Log Return means\n",
    "ax1.plot(intraday_profile['minute_of_day'], intraday_profile[('log_return', 'mean')], \n",
    "         color='blue', linewidth=2, label='Mean log_return')\n",
    "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='Origin (Zero)')\n",
    "ax1.set_title('[Avg. log_volume per min]', fontsize=14, loc='left')\n",
    "ax1.set_xlabel('Minute of the day')\n",
    "ax1.set_ylabel('Avg. log_return')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Market hours of NY Stock Exchange in eastern timezone \n",
    "market_times = {\n",
    "    570: '9:30 (Opening)',\n",
    "    720: '12:00 (Noon)', \n",
    "    960: '16:00 (Closing)'\n",
    "}\n",
    "\n",
    "# Plot 2: Log Volume means\n",
    "ax2.plot(intraday_profile['minute_of_day'], intraday_profile[('log_volume', 'mean')], \n",
    "         color='green', linewidth=2, label='Mean log_volume')\n",
    "ax2.set_title('[Avg. log_volume per min]', fontsize=14, loc='left')\n",
    "ax2.set_xlabel('Minute of the day')\n",
    "ax2.set_ylabel('Avg. log_volume')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "# Mark market hours in both plots\n",
    "for ax in [ax1, ax2]:\n",
    "    for minute, label in market_times.items():\n",
    "        if minute in intraday_profile['minute_of_day'].values:\n",
    "            ax.axvline(x=minute, color='gray', linestyle=':', alpha=0.6)\n",
    "            y_pos = ax.get_ylim()[0] + (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.9\n",
    "            ax.text(minute, y_pos, label, fontsize=8, ha='right')\n",
    "            \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(f\"[Mean Stats] Intraday Profile of means of TSLA Stock Data\")\n",
    "print(f\"Log Return:\")\n",
    "print(f\"  - Min: {intraday_profile[('log_return', 'mean')].min():.6f}\")\n",
    "print(f\"  - Max: {intraday_profile[('log_return', 'mean')].max():.6f}\")\n",
    "print(f\"  - Span: {intraday_profile[('log_return', 'mean')].max() - intraday_profile[('log_return', 'mean')].min():.6f}\")\n",
    "\n",
    "print(f\"Log Volume:\")\n",
    "print(f\"  - Min: {intraday_profile[('log_volume', 'mean')].min():.3f}\")\n",
    "print(f\"  - Max: {intraday_profile[('log_volume', 'mean')].max():.3f}\")\n",
    "print(f\"  - Span: {intraday_profile[('log_volume', 'mean')].max() - intraday_profile[('log_volume', 'mean')].min():.3f}\")\n",
    "\n",
    "min_volume_time = intraday_profile.loc[intraday_profile[('log_volume', 'mean')].idxmin(), 'time_label']\n",
    "max_volume_time = intraday_profile.loc[intraday_profile[('log_volume', 'mean')].idxmax(), 'time_label']\n",
    "\n",
    "print(f\"\\n[Peak Times]\")\n",
    "print(f\"Lowest volume at: {min_volume_time.iloc[0]}\")\n",
    "print(f\"Highest volume at: {max_volume_time.iloc[0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44f03aa7",
   "metadata": {},
   "source": [
    "### Plotting volatility (std) of log return / volume and perform F-test ###\n",
    "\n",
    "print(\"[Intraday Profile] Visualization of intraday seasonality: Volatility (std) per minute of day\")\n",
    "\n",
    "# Subplot with two y-axes for different scales\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Log Return volatility\n",
    "ax1.plot(intraday_profile['minute_of_day'], intraday_profile[('log_return', 'std')], \n",
    "         color='red', linewidth=2, label='Volatility log_return (Std.)')\n",
    "ax1.set_title('[Standard Deviation log_return per Minute]', fontsize=14, loc='left')\n",
    "ax1.set_xlabel('Minute of the day')\n",
    "ax1.set_ylabel('Std. log_return')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Log Volume Volatility\n",
    "ax2.plot(intraday_profile['minute_of_day'], intraday_profile[('log_volume', 'std')], \n",
    "         color='orange', linewidth=2, label='Volatility log_volume (Std.)')\n",
    "ax2.set_title('[Standard Deviation log_volume per Minute]', fontsize=14, loc='left')\n",
    "ax2.set_xlabel('Minute of the day')\n",
    "ax2.set_ylabel('Std. log_volume')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "# Mark market hours in both plots\n",
    "for ax in [ax1, ax2]:\n",
    "    for minute, label in market_times.items():\n",
    "        if minute in intraday_profile['minute_of_day'].values:\n",
    "            ax.axvline(x=minute, color='gray', linestyle=':', alpha=0.6)\n",
    "            y_pos = ax.get_ylim()[0] + (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.9\n",
    "            ax.text(minute, y_pos, label, fontsize=8, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Volatility stats\n",
    "print(f\"[Volatility Stats] Intraday Profile of Volatility (Std.) of TSLA Stock Data\")\n",
    "print(f\"Log Return Volatility:\")\n",
    "print(f\"  - Minimum Std: {intraday_profile[('log_return', 'std')].min():.6f}\")\n",
    "print(f\"  - Maximum Std: {intraday_profile[('log_return', 'std')].max():.6f}\")\n",
    "print(f\"  - Proportion Max/Min: {intraday_profile[('log_return', 'std')].max() / intraday_profile[('log_return', 'std')].min():.2f}x\")\n",
    "\n",
    "print(f\"\\nLog Volume Volatility:\")\n",
    "print(f\"  - Minimum Std: {intraday_profile[('log_volume', 'std')].min():.3f}\")\n",
    "print(f\"  - Maximum Std: {intraday_profile[('log_volume', 'std')].max():.3f}\")\n",
    "print(f\"  - Proportion Max/Min: {intraday_profile[('log_volume', 'std')].max() / intraday_profile[('log_volume', 'std')].min():.2f}x\")\n",
    "\n",
    "min_vol_return_time = intraday_profile.loc[intraday_profile[('log_return', 'std')].idxmin(), 'time_label']\n",
    "max_vol_return_time = intraday_profile.loc[intraday_profile[('log_return', 'std')].idxmax(), 'time_label']\n",
    "\n",
    "print(f\"\\n[Peak Times]\")\n",
    "print(f\"Lowest Return-Volatility at: {min_vol_return_time.iloc[0]}\")\n",
    "print(f\"Highest Return-Volatility at: {max_vol_return_time.iloc[0]}\")\n",
    "\n",
    "# Test for Seasonality\n",
    "# Group data by hours and test for differences\n",
    "df_stock_data['hour'] = df_stock_data['minute_of_day'] // 60\n",
    "hourly_returns = [group['log_return'].dropna() for name, group in df_stock_data.groupby('hour')]\n",
    "hourly_volumes = [group['log_volume'].dropna() for name, group in df_stock_data.groupby('hour')]\n",
    "\n",
    "# F-test for equality of variances over the hours\n",
    "if len(hourly_returns) > 2:  # At least 3 groups for F-Test, * marks a series of groups\n",
    "    f_stat_returns, p_val_returns = f_oneway(*hourly_returns)\n",
    "    f_stat_volumes, p_val_volumes = f_oneway(*hourly_volumes)\n",
    "    \n",
    "    print(f\"\\n[Statistical Test for Intraday Seasonality]\")\n",
    "    print(f\"F-Test for log_return for hours: F={f_stat_returns:.3f}, p={p_val_returns:.6f}\")\n",
    "    print(f\"F-Test for log_volume for hours: F={f_stat_volumes:.3f}, p={p_val_volumes:.6f}\")\n",
    "    \n",
    "    alpha = 0.05\n",
    "    print(f\"\\nWith Œ± = {alpha}:\")\n",
    "    print(f\"Detected significant intraday differences for log_returns: {'Yes' if p_val_returns < alpha else 'No'}\")\n",
    "    print(f\"Detected significant intraday differences for log_volumes: {'Yes' if p_val_volumes < alpha else 'No'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30f5c149",
   "metadata": {},
   "source": [
    "<p><i>Intraday Seasonality Test Results</i></p>\n",
    "<p>Log_return\n",
    "    <ul>\n",
    "        <li>Results: No significant intraday seasonality observable</li>\n",
    "        <li>Implications: No need to incorporate the time of day in our analysis of the log return</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>Log_volume\n",
    "    <ul>\n",
    "        <li>Results: Significant intraday seasonality observable - time of the day has effect on trading volume</li>\n",
    "        <li>Implications: Must include the effect into our analysis</li>\n",
    "        <li>How: intraday z-normalization of log_volume</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7be738",
   "metadata": {},
   "source": [
    "<h3>[2.4][Normalization]</h3>\n",
    "<p>\n",
    "    To evaluate whether an event has a significant effect on the return / volume, we must check if the mean from the event-window (point in time or timeframe) significantly differentiates from the mean of the estimation window, which we use as baseline for comparison. This can be done by performing a t-test, however, this presumes the variable to be normally distributed.\n",
    "</p>\n",
    "<p>\n",
    "    By plotting the log_return and log_volume we can see, that the log_return already is approximately normally distributed. The log_volume however incorporates the seasonality. By observing the two peaks we can tell that more extreme values occur more frequently and that consequently the variable is not normally distributed (can be lead back to higher volume at beginning / end of trading day).\n",
    "</p>\n",
    "<p>\n",
    "    To even out the intraday seasonality we must offset each log_volume against those that occur at the exact same time on different days. Hence, we must group the data by time of the day first and use this groups mean and standard deviation to perform a intraday z-normalization. By doing so, we receive a plot that nicely shows a normally distributed variable (code above, plot below).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "id": "de6258f9",
   "metadata": {},
   "source": [
    "# Show histplot with curve of log returns and intraday z-normalized log volumes\n",
    "## log_return\n",
    "print(\"Log Return]\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df_stock_data['log_return'].dropna(), bins=50, kde=True)\n",
    "plt.title('Histogramm of log returns')\n",
    "plt.xlabel('log_return')\n",
    "plt.xlim([-0.05, 0.05])\n",
    "plt.ylabel('Occurences')\n",
    "plt.show()\n",
    "\n",
    "## log_volume\n",
    "print(\"[Log Volume]\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df_stock_data['log_volume'].dropna(), bins=50, kde=True)\n",
    "plt.title('Histogramm of log volumes')\n",
    "plt.xlabel('log_volume')\n",
    "plt.xlim([0, 17.5])\n",
    "plt.ylabel('Occurences')\n",
    "plt.show()\n",
    "\n",
    "## log_volume_z_intraday\n",
    "print(\"[z-Normalized Intraday Log Volume]\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df_stock_data['log_volume_z_intraday'].dropna(), bins=50, kde=True)\n",
    "plt.title('Histogram of intraday z-normalized log volumes')\n",
    "plt.xlabel('log_volume_z_intraday')\n",
    "plt.xlim([-5, 5])\n",
    "plt.ylabel('Occurences')\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ada296b",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <i>Implications of Normalization</i>\n",
    "</p>\n",
    "<p>\n",
    "    In general, the z-transformation makes it possible to compare a single volume to all other volumes, independent of the time of the day. The intraday z-transformation however also takes the volatility of the time of the day into account and comares a single volume, that occurs at a certain time of the day, to all other volumes that occured at the same time of the day. This makes it possible to find effects beyond the temporal volatility. However, when interpreting the results, we must remember that each temporal group has its own standard deviation. \n",
    "</p>\n",
    "<p>\n",
    "    As the log_return already is approximately normally distributed, we do not need to normalize manually and can compute the metrics and compare it to the baseline (estimation window). \n",
    "    The log_volume still inherits the intraday effects however. To eliminate this bias, we must group the log volumes by the minute of the day and z-normalize based on this groups mean and standard deviation. By doing so, it is now more difficult to compute the CAV, AAV, and CAAV. More on this later...\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97600098f6f929",
   "metadata": {},
   "source": [
    "<h1>Emotion Analysis - Part 3</h1>\n",
    "<h2><i>Event Study</i></h2>\n",
    "<h3>[3.1][Data Preparation]</h3>\n",
    "<p>\n",
    "    Grouping tweets to events and assigning a weighted event emotion.\n",
    "</p>\n",
    "<ul>\n",
    "    <li>Filter stock data by market hours</li>\n",
    "    <li>Group tweets with diff < 7h to an event</li>\n",
    "    <li>Assign weighed event emotions based on engagement</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "id": "e50459a2203b6c91",
   "metadata": {},
   "source": [
    "### Group tweets to events with dominant emotion ###\n",
    "\n",
    "## Pre-process tweets for event study\n",
    "# 1. Filter out tweets outside market hours (+-2h)\n",
    "df_tweets_normalized = df_tweets_normalized[\n",
    "    df_tweets_normalized['timestamp'].dt.time >= pd.to_datetime(\"04:00\").time()\n",
    "]\n",
    "df_tweets_normalized = df_tweets_normalized[\n",
    "    df_tweets_normalized['timestamp'].dt.time <= pd.to_datetime(\"17:59\").time()\n",
    "]\n",
    "\n",
    "# 2. Sort by time\n",
    "df_tweets_normalized = df_tweets_normalized.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# 3. Compute time difference between tweets\n",
    "df_tweets_normalized['time_diff'] = df_tweets_normalized['timestamp'].diff()\n",
    "\n",
    "# 4. Cluster Tweets to events with time_diff >= 7h\n",
    "df_tweets_normalized['new_event'] = df_tweets_normalized['time_diff'] > pd.Timedelta(hours=7)\n",
    "\n",
    "# 5. Cumulate events in event_id\n",
    "df_tweets_normalized['event_id'] = df_tweets_normalized['new_event'].cumsum()\n",
    "\n",
    "# 6. Compute the dominant emotion for each event and merge back to df\n",
    "def compute_event_emotion(group):\n",
    "    total_engagement = group['engagement'].sum()\n",
    "    weighted_emo_scores = {\n",
    "        emotion: (group[emotion] * group['engagement']).sum() / total_engagement\n",
    "        for emotion in ekman_emotions\n",
    "    }\n",
    "    event_emotion = max(weighted_emo_scores, key=weighted_emo_scores.get)\n",
    "    return pd.Series({\n",
    "        **weighted_emo_scores,\n",
    "        'event_emotion': event_emotion\n",
    "    })\n",
    "\n",
    "if not ('event_emotion' in df_tweets_normalized.columns):\n",
    "    df_event_emotions = (df_tweets_normalized\n",
    "        .drop(columns=['event_id'])\n",
    "        .groupby(df_tweets_normalized['event_id'], group_keys=False)\n",
    "        .apply(compute_event_emotion)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    df_tweets_normalized = (df_tweets_normalized\n",
    "        .merge(\n",
    "            df_event_emotions[['event_id', 'event_emotion']],\n",
    "            on='event_id',\n",
    "            how='left'\n",
    "        )\n",
    "        .drop(columns={'anger', 'fear', 'joy', 'sadness', 'disgust', 'surprise', 'tweet_emotion', 'new_event'})\n",
    "    )\n",
    "df_tweets_normalized.head()\n",
    "\n",
    "# 8. Set event from time to time of the earliest tweet (models from what point in time a reaction can occur)\n",
    "df_tweets_normalized['event_time'] = df_tweets_normalized.groupby('event_id')['timestamp'].transform('min')\n",
    "\n",
    "# 9. Filter duplicate events\n",
    "df_tweets_normalized = df_tweets_normalized.drop_duplicates(subset=['event_id'])\n",
    "\n",
    "print(\"[Info] Number of unique events = \", df_tweets_normalized['event_id'].nunique(), \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_tweets_normalized.to_csv('../data/twitter/tweets_isTweet_emotions.csv', index=False)",
   "id": "3fc071d536abe00b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c30f8f9c",
   "metadata": {},
   "source": [
    "<h3>[3.1][Create Event Study Dataframes]</h3>\n",
    "<p>\n",
    "    In this step, we eliminate some events that do not fulfill the following criteria. Hence, the number of unique events will be reduced. The data is filtered/processed as follows:\n",
    "</p>\n",
    "<ul>\n",
    "    <li>Event timestamps are aligned with the subsequent stock data timestamp. Hence, they are rounded and assigned to the next full minute.</li>\n",
    "    <li>Events that do not have a full 5h history of estimation trading data are sorted out.</li>\n",
    "    <li>Events that do not have a full 2h post-event window of trading data ahead are sorted out.</li>\n",
    "    <li>Events, whose estimation or event window exceed the trading day, are sorted out (in alignment to the previous two points).</li>\n",
    "    <li>Additionally, the <u>abnormal_return</u> and <u>abnormal_volume</u> is computed (also intraday z-normalized)</li>\n",
    "</ul>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Reading Event Study Dataframes ###\n",
    "df_event_study = pd.read_csv('../data/twitter/emotion_event_study.csv')\n",
    "df_event_study_pre_post = pd.read_csv('../data/twitter/emotion_event_study_pre_post.csv')"
   ],
   "id": "52443c9588639340",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Create/Read Event Study DataFrame (with actual, expected, abnormal return and volume) ###\n",
    "\n",
    "# 1. Filter events based on overlaps, estimation and observation windows\n",
    "# 2. Calculate expected and abnormal returns for each event\n",
    "def filter_events(df, get_pre = 0):\n",
    "    # Define necessary vars\n",
    "    est = 300\n",
    "    obs = 120\n",
    "    gap = est + obs\n",
    "    results = []\n",
    "\n",
    "    # 1. Filter by Overlaps\n",
    "    valid_events = []\n",
    "    last_event = None\n",
    "\n",
    "    # Add valid events to the list\n",
    "    for date in df.index:\n",
    "        # First event always valid\n",
    "        if last_event is None:\n",
    "            valid_events.append(date)\n",
    "            last_event = date\n",
    "        else:\n",
    "            # If timeshift > gaps, keep the current event and set as new last_event\n",
    "            if (date - last_event) > pd.Timedelta(minutes=gap):\n",
    "                valid_events.append(date)\n",
    "                last_event = date\n",
    "\n",
    "    # Only keep valid events\n",
    "    df = df.loc[valid_events]\n",
    "    print(f\"[Overlap] New df_length = {len(df)}\")\n",
    "\n",
    "    # 2. Align event_time with the next available trading timestamp\n",
    "    df.index = df.index.tz_convert(eastern)\n",
    "\n",
    "    for i, date in enumerate(df.index):\n",
    "        if date not in df_trades.index:\n",
    "            future_times = df_trades.index[df_trades.index > date]\n",
    "            if not future_times.empty:\n",
    "                df.index.values[i] = pd.Timestamp(future_times[0])\n",
    "\n",
    "    # 3. Filter events without sufficient estimation_window data\n",
    "    # 4. Filter events without sufficient event_window data\n",
    "    valid_events = []\n",
    "\n",
    "    for date in df.index:\n",
    "        day_data = df_trades[df_trades.index.date == date.date()]\n",
    "        event_index = day_data.index.get_loc(date)\n",
    "\n",
    "        remaining = len(day_data) - event_index\n",
    "\n",
    "        if event_index >= est:\n",
    "            if remaining >= obs:\n",
    "                valid_events.append(date)\n",
    "\n",
    "    df = df.loc[valid_events]\n",
    "    print(f\"[Sum Data] New df_length = {len(df)}\")\n",
    "\n",
    "    # 5. Event study variables\n",
    "    for event_row in df.itertuples():\n",
    "        event_id = event_row.event_id\n",
    "        event_emo = event_row.event_emotion\n",
    "        event_time = event_row.Index\n",
    "\n",
    "        estimation_window = df_trades.loc[\n",
    "            event_time - pd.Timedelta(minutes=est) : event_time - pd.Timedelta(minutes=1)\n",
    "        ]\n",
    "        event_window = df_trades.loc[\n",
    "            event_time - pd.Timedelta(minutes=get_pre * est): event_time + pd.Timedelta(minutes=obs - 1)\n",
    "        ]\n",
    "\n",
    "        expected_return = estimation_window['log_return'].mean()\n",
    "        expected_volume = estimation_window['log_volume'].median() # robust against outliers, see Bamber (1987)\n",
    "        expected_volume_z_intraday = estimation_window['log_volume_z_intraday'].median()\n",
    "\n",
    "        for i, fin_row in enumerate(event_window.itertuples()):\n",
    "            if get_pre == 1:\n",
    "                i = int((fin_row.Index - event_time).total_seconds() // 60)\n",
    "\n",
    "            actual_return = fin_row.log_return\n",
    "            abnormal_return = actual_return - expected_return\n",
    "\n",
    "            actual_volume = fin_row.log_volume\n",
    "            abnormal_volume = actual_volume - expected_volume\n",
    "\n",
    "            actual_volume_z_intraday = fin_row.log_volume_z_intraday\n",
    "            abnormal_volume_z_intraday = actual_volume_z_intraday - expected_volume_z_intraday\n",
    "\n",
    "            results.append({\n",
    "                'event_id': event_id,\n",
    "                'event_fin_offset': i,\n",
    "                'event_time': event_time,\n",
    "                'fin_time': fin_row.Index,\n",
    "                'event_emo': event_emo,\n",
    "                'actual_return': actual_return,\n",
    "                'expected_return': expected_return,\n",
    "                'abnormal_return': abnormal_return,\n",
    "                'actual_volume': actual_volume,\n",
    "                'expected_volume': expected_volume,\n",
    "                'abnormal_volume': abnormal_volume,\n",
    "                'actual_volume_z_intraday': actual_volume_z_intraday,\n",
    "                'expected_volume_z_intraday': expected_volume_z_intraday,\n",
    "                'abnormal_volume_z_intraday': abnormal_volume_z_intraday\n",
    "            })\n",
    "\n",
    "    # Make a dataframe from the result list\n",
    "    df_valid_events = pd.DataFrame(results)\n",
    "\n",
    "    return df_valid_events"
   ],
   "id": "4cdc73cf9821fd56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if RUN_TYPE == 1: # if not yet created, create the event study dataframes with abnormal returns and volumes\n",
    "\n",
    "    #  Create new dfs with necessary event data and set time column as index\n",
    "    df_events = (df_tweets_normalized[['event_id', 'event_emotion', 'event_time']]\n",
    "                .set_index('event_time')\n",
    "                .sort_index())\n",
    "    df_trades = (df_stock_data[['timestamp', 'log_return', 'log_volume', 'log_volume_z_intraday']]\n",
    "                .set_index('timestamp')\n",
    "                .sort_index())\n",
    "\n",
    "    df_event_study = filter_events(df_events, get_pre = 0).to_csv('../data/twitter/emotion_event_study.csv', index=False)\n",
    "    df_event_study_pre_post = filter_events(df_events, get_pre=1).to_csv('../data/twitter/emotion_event_study_pre_post.csv', index=False)\n",
    "\n",
    "print(\"[Info] Dataframe Inspection\")\n",
    "print(f\"Emotions: {df_event_study['event_emo'].unique()}\")\n",
    "emotion = df_event_study['event_emo'].unique()\n",
    "for emo in emotion:\n",
    "    print(f\"  - {emo}: {len(df_event_study[df_event_study['event_emo'] == emo].groupby('event_id'))} events\")\n",
    "\n",
    "print(f\"\\n[Info] Event Study DataFrame has {len(df_event_study)} entries.\")\n",
    "print(f\"[Info] Event Study Pre/Post DataFrame has {len(df_event_study_pre_post)} entries.\\n\")\n",
    "if (df_event_study_pre_post.columns == df_event_study.columns).all():\n",
    "    print(\"[Info] emotion_event_study(_pre_post).csv files have the following columns:\")\n",
    "    print(df_event_study.columns.tolist())"
   ],
   "id": "55aaa22a9012dcc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<p><i>Resulting Dataframe</i></p>\n",
    "<p>\n",
    "    We get a dataframe that contains all events with their corresponding event_window (and estimation_window) tuples, containing offset and abnormal returns / volumes. With this, we can now compute our event study metrics.\n",
    "</p>"
   ],
   "id": "c4e196c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>[3.2][Event Study on Return]</h3>\n",
   "id": "a4a299a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4><u>3.2.1 Abnormal Return</u></h4>\n",
    "<p>[Intro]</p>\n",
    "<p>In the following it is analysed to what extent a significant effect of a particular emotion on the change in stock return can be observed by the minute<br>\n",
    "--> Is the mean abnormal return significantly greater / lower than 0 resp. does the mean abnormal return significantly differ from the expected return?</p>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>H0: mean abnormal return = 0</li>\n",
    "        <li>H1: mean abnormal return >< 0</li>\n",
    "    </ul>\n",
    "</p>"
   ],
   "id": "38f2e3ae92dcbcf0"
  },
  {
   "cell_type": "code",
   "id": "9b450ea1",
   "metadata": {},
   "source": [
    "### Significance Testing of AR ###\n",
    "\n",
    "print(f\"[Info] Testing for significant effect of emotions {df_event_study['event_emo'].unique()} on abnormal returns ...\")\n",
    "\n",
    "## Functions\n",
    "def stars(p):\n",
    "    if p < 0.001: return '***'\n",
    "    elif p < 0.01: return '**'\n",
    "    elif p < 0.05: return '*'\n",
    "    else: return ''\n",
    "    \n",
    "def test_abnormal_returns(df, test_col):\n",
    "    results = []\n",
    "\n",
    "    emotions = df['event_emo'].unique()\n",
    "    minutes = df['event_fin_offset'].unique()\n",
    "\n",
    "    for minute in minutes:\n",
    "        row = {'minute': minute}\n",
    "\n",
    "        for emo in emotions:\n",
    "            values = df.loc[\n",
    "                (df['event_fin_offset'] == minute) & (df['event_emo'] == emo),\n",
    "                test_col\n",
    "            ]\n",
    "            if len(values) > 1:\n",
    "                t_stat, p_value = stats.ttest_1samp(values, 0)\n",
    "                row[f'{emo}_{test_col}_mean'] = values.mean()\n",
    "                row[f'{emo}_{test_col}_t_stat'] = t_stat\n",
    "                row[f'{emo}_{test_col}_p_value'] = p_value\n",
    "                row[f'{emo}_{test_col}_sig'] = f\"{t_stat:.2f}{stars(p_value)}\" # adds stars \n",
    "            else:\n",
    "                row[f'{emo}_{test_col}_mean'] = np.nan\n",
    "                row[f'{emo}_{test_col}_t_stat'] = np.nan\n",
    "                row[f'{emo}_{test_col}_p_value'] = np.nan\n",
    "                row[f'{emo}_{test_col}_sig'] = ''\n",
    "        \n",
    "        results.append(row)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "## Test abnormal returns and visualize results\n",
    "# Test the abnormal returns for each emotion and minute\n",
    "ar_test_scores = test_abnormal_returns(df_event_study, 'abnormal_return')\n",
    "ar_test_scores.set_index('minute', inplace=True)\n",
    "\n",
    "# Show only rows with significant p-values\n",
    "alpha = 0.05\n",
    "\n",
    "# Make list of p-value columns\n",
    "pval_cols = [col for col in ar_test_scores.columns if col.endswith('_p_value')]\n",
    "\n",
    "# Keep only rows where at least one p-value is below alpha\n",
    "significant_rows = ar_test_scores[(ar_test_scores[pval_cols] < alpha).any(axis=1)]\n",
    "\n",
    "# Show rows that contain a significant p-value\n",
    "sig_cols = [col for col in significant_rows.columns if col.endswith('_sig')]\n",
    "significant_rows[sig_cols]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8181469e",
   "metadata": {},
   "source": [
    "### Plot Heatmap of t-Statistics for Abnormal Returns ###\n",
    "\n",
    "# Emotionen extrahieren (alle Spalten mit '_t_stat' am Ende)\n",
    "emotions = [col.replace('_t_stat', '') for col in ar_test_scores.columns if col.endswith('_t_stat')]\n",
    "\n",
    "# DataFrame f√ºr Heatmap vorbereiten: Zeilen = Emotionen, Spalten = Minuten\n",
    "heatmap_data = pd.DataFrame({\n",
    "    emo: ar_test_scores[f\"{emo}_t_stat\"] for emo in emotions\n",
    "}).T  # Transponieren: Emotionen als Zeilen\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    annot=False,\n",
    "    cbar_kws={'label': 't-Statistik'}\n",
    ")\n",
    "plt.xlabel(\"Minute nach Event\")\n",
    "plt.ylabel(\"Emotion\")\n",
    "plt.title(\"Heatmap der t-Statistiken der abnormal returns pro Emotion und Minute\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b7390346",
   "metadata": {},
   "source": [
    "<p>[Test-Results]</p>\n",
    "<p><i>What was tested?</i></p>\n",
    "<p>For each emotion and each minute in the post-event window it was tested if the mean abnormal return significantly deviates from 0. The goal was to detect whether there is a deviation from the expected return (H0: mean = 0).</p>\n",
    "<p><i>Results</i></p>\n",
    "<p>The heatmap shows that only for few points across the emotions, a significant effect could be observed, where the mean abnormal return significantly deviates from 0. Thus, the H1: mean >< 0 can most likely be refused.\n",
    "    <ul>\n",
    "        <li>No consistent pattern of significant observations visible, that may indicate a recurrent effect.</li>\n",
    "        <li>There are a few significant events across the heatmap. Because of their low density they can probably be declared as outliers.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p><i>Conclusion</i></p>\n",
    "<p>In the short-term and for individual points in time, an impact of a tweets emotion on the expected return of the stock can be neglected.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b9b9a",
   "metadata": {},
   "source": [
    "<h4><u>3.2.2 Cumulative Abnormal Return</u></h4>\n",
    "<p>\n",
    "    [Intro]\n",
    "</p>\n",
    "<p>\n",
    "    In the following it is analysed, whether an effect occurs within specified time windows. As log returns are additive, we can use the cumsum() Method to compute the cumulative sum and plot it over time. This way, we can yield a graph that shows the actual return over time and how the abnormal return deviates after the event.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "id": "7daed245",
   "metadata": {},
   "source": [
    "### [All emotions] Plotting Cumulative Log Returns Around Tweets by emotion ###\n",
    "\n",
    "# Present emotions in the DataFrame\n",
    "emotions = df_event_study_pre_post['event_emo'].unique()\n",
    "\n",
    "for emo, i in zip(emotions, range(len(emotions))):\n",
    "\n",
    "    # 0. Windows\n",
    "    estimation_window = df_event_study_pre_post\n",
    "    event_window = df_event_study_pre_post[df_event_study_pre_post['event_fin_offset'] >= 0]\n",
    "\n",
    "    # 0.5 Plot Overall Cumulative Log Returns Around Tweets \n",
    "    if i == 0:\n",
    "        actual_returns = estimation_window.groupby('event_fin_offset')['actual_return'].mean()\n",
    "        cumulative_actual_return = actual_returns.cumsum()\n",
    "\n",
    "        abnormal_returns = event_window.groupby('event_fin_offset')['abnormal_return'].mean()\n",
    "        cumulative_abnormal_return = abnormal_returns.cumsum()\n",
    "\n",
    "        print(\"[Overall]\")\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(cumulative_actual_return.index, cumulative_actual_return.values * 100, label=\"Actual Log Return (%)\", color=\"blue\")\n",
    "        plt.plot(cumulative_abnormal_return.index, cumulative_abnormal_return.values * 100, label=\"Abnormal Log Return (%)\", color=\"orange\")\n",
    "        plt.axvline(0, linestyle=\"--\", color=\"black\", label=\"Tweet\")\n",
    "        plt.axhline(0, linestyle=\"-\", color=\"gray\")\n",
    "        plt.title(\"Cumulative Log Returns Around Tesla Tweets (Overall)\")\n",
    "        plt.xlabel(\"Minute Offset from Tweet\")\n",
    "        plt.xticks([-300, -240, -180, -120, -60, 0, 60, 120])\n",
    "        plt.ylabel(\"Cumulative Return (%)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"-----------------------------------------------------------------------------------------------------------------\\n\")\n",
    "        print(\"[Emotions]\\n\")\n",
    "\n",
    "    # 1. Calculate cumulative actual log returns around tweets\n",
    "    actual_returns = estimation_window[estimation_window['event_emo'] == emo].groupby('event_fin_offset')['actual_return'].mean()\n",
    "    cumulative_actual_return = actual_returns.cumsum()\n",
    "\n",
    "    # 2. Caluculate cumulative abnormal log returns around tweets\n",
    "    abnormal_returns = event_window[event_window['event_emo'] == emo].groupby('event_fin_offset')['abnormal_return'].mean()\n",
    "    cumulative_abnormal_return = abnormal_returns.cumsum()\n",
    "\n",
    "    # 3. Plot\n",
    "    # Print CAR over the event window\n",
    "    caar = abnormal_returns.sum()  # Sum of mean abnormal returns over the event window (caar)\n",
    "    print(f\"[Plot] Plotting cumulative log returns for emotion: {emo}\")\n",
    "    print(f\"[Info] Cumulative Average Abnormal Return (CAAR) for {emo} over the event window: {caar* 100:.4f} %\\n\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cumulative_actual_return.index, cumulative_actual_return.values * 100, label=\"Actual Log Return (%)\", color=\"blue\")\n",
    "    plt.plot(cumulative_abnormal_return.index, cumulative_abnormal_return.values * 100, label=\"Abnormal Log Return (%)\", color=\"orange\")\n",
    "    plt.axvline(0, linestyle=\"--\", color=\"black\", label=\"Tweet\")\n",
    "    plt.axhline(0, linestyle=\"-\", color=\"gray\")\n",
    "    plt.title(\"Cumulative Log Returns Around Tesla Tweets (Emotion: \" + emo + \")\")\n",
    "    plt.xlabel(\"Minute Offset from Tweet\")\n",
    "    plt.xticks([-300, -240, -180, -120, -60, 0, 60, 120])\n",
    "    plt.ylabel(\"Cumulative Return (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ae3ff78",
   "metadata": {},
   "source": [
    "### Test for significant effect of emotions on abnormal volume in window ###\n",
    "\n",
    "windows = [(0, 1), (0, 2), (0, 5), (0, 10), (0, 30), (0, 60), (0, 120)]\n",
    "\n",
    "results = []\n",
    "for emo, i in zip(emotions, range(len(emotions))):\n",
    "    # Iterate through each time window and test for significant effects\n",
    "    for start, end in windows:\n",
    "\n",
    "        # Here: testing overall effect \n",
    "        if i == 0:\n",
    "            # Filter time window without emotion\n",
    "            window = df_event_study[\n",
    "                (df_event_study['event_fin_offset'] >= start) &\n",
    "                (df_event_study['event_fin_offset'] <= end)\n",
    "            ]    \n",
    "            \n",
    "            # Compute CAR for each event\n",
    "            event_car = window.groupby('event_time')['abnormal_return'].sum()\n",
    "\n",
    "            # Calculate mean CAR across all events\n",
    "            mean_car = event_car.mean() \n",
    "\n",
    "            # Perform t-test against zero (significantly different from zero)\n",
    "            t_stat, p_value = stats.ttest_1samp(event_car, 0)\n",
    "\n",
    "            results.append({\n",
    "                'emotion': 'NaN',\n",
    "                'window': f\"[{start}, {end}]\",\n",
    "                'mean_car': f\"{mean_car * 100:.4f} %\",  # Convert to percentage\n",
    "                't_stat': f\"{round(t_stat, 2)}{stars(p_value)}\",  # Add stars for significance\n",
    "            })\n",
    "\n",
    "        # Here: testing for specific emotion\n",
    "        else:   \n",
    "            # Filter for the current emotion and time window\n",
    "            emo_window = df_event_study[\n",
    "                (df_event_study['event_emo'] == emo) &\n",
    "                (df_event_study['event_fin_offset'] >= start) &\n",
    "                (df_event_study['event_fin_offset'] <= end)\n",
    "            ]\n",
    "        \n",
    "            # Compute CAR for each event\n",
    "            event_car = emo_window.groupby('event_time')['abnormal_return'].sum()\n",
    "\n",
    "            # Calculate mean CAR across all events\n",
    "            mean_car = event_car.mean() \n",
    "\n",
    "            # Perform t-test against zero (significantly different from zero)\n",
    "            t_stat, p_value = stats.ttest_1samp(event_car, 0)\n",
    "\n",
    "            results.append({\n",
    "                'emotion': emo,\n",
    "                'window': f\"[{start}, {end}]\",\n",
    "                'mean_car': f\"{mean_car * 100:.4f} %\",  # Convert to percentage\n",
    "                't_stat': f\"{round(t_stat, 2)}{stars(p_value)}\",  # Add stars for significance\n",
    "            })\n",
    "\n",
    "df_car_results = pd.DataFrame(results)\n",
    "\n",
    "for emo in df_car_results['emotion'].unique():\n",
    "    print(f\"\\n[Info] Results for emotion '{emo}':\")\n",
    "    emo_results = df_car_results[df_car_results['emotion'] == emo]\n",
    "    print(emo_results[['window', 'mean_car', 't_stat']])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "030ff3d1",
   "metadata": {},
   "source": [
    "<p>[Test-Results]</p>\n",
    "<p><i>What was tested?</i></p>\n",
    "<p>For each emotion and each minute in the post-event window it was tested if the mean abnormal return significantly deviates from 0. The goal was to detect whether there is a deviation from the expected return (H0: mean = 0).</p>\n",
    "<p><i>Results</i></p>\n",
    "<p>The heatmap shows that only for few points across the emotions, a significant effect could be observed, where the mean abnormal return significantly deviates from 0. Thus, the H1: mean >< 0 can most likely be refused.\n",
    "    <ul>\n",
    "        <li>No consistent pattern of significant observations visible, that may indicate a recurrent effect.</li>\n",
    "        <li>There are a few significant events across the heatmap. Because of their low density they can probably be declared as outliers.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p><i>Conclusion</i></p>\n",
    "<p>In the short-term and for individual points in time, an impact of a tweets emotion on the expected return of the stock can be neglected.</p>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>[3.3][Event Study on Volume]</h3>\n",
    "<p><i>WILL BE PERFORMED LATER IF THERE IS STILL TIME</i></p>"
   ],
   "id": "4a069faa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4><u>3.3.1 Abnormal Volume</u></h4>\n",
    "<p>In the following it is analysed to what extent a significant effect of a particular emotion on the change in trade volume can be observed by the minute<br>\n",
    "--> Is the mean abnormal volume significantly greater / lower than 0 resp. does the mean abnormal volume significantly differ from the expected volume?</p>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>H0: mean abnormal volume = 0</li>\n",
    "        <li>H1: mean abnormal volume >< 0</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>Here, the absolute trading volume is logarithmized for each offset. However, as volume is interesting as an absolute value (how many stocks where traded), the value is always positive and may deviate a lot. Hence, the log_volume is not normally distributed (see Illustration below) and a t-test cannot be performed as it assumes a normal distribution of the variable.</p>\n",
    "<p>Consequently, a z-transformation is performed on the log_volume to be able to properly test for significance.</p>"
   ],
   "id": "0c47684b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
